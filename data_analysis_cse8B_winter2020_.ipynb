{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyS7UsaE_2kN"
   },
   "source": [
    "# INTRODUCTION (Main Questions)\n",
    "\n",
    "This is the data analysis notebook for CSE 8B Winter 2020\n",
    "\n",
    "\n",
    "<hr> \n",
    "\n",
    "## <a id='toc'>Table of Contents </a>\n",
    "\n",
    "- [Setting up data](#section_sud)\n",
    "- [Basic Cleaning data](#section_2)\n",
    "- <b> [General Functions](#section_general_functions)</b> \n",
    "- [Missing Data Explanation](#section_missing_data)\n",
    "- [How We Handle Outliers](#section_outlier) \n",
    "- [List of Dataframes](#section_dfs)\n",
    "- [General Mean Functions](#section_funcs_mean) \n",
    "- <b> [Research Question Start (On Hours spent outside of class)](#section_RQ1) </b> \n",
    "- [Research Question - Hours Spent Outside of Class Graphs](#section_RQ1_graphs) </b> \n",
    "- [Research Question Conclusion (On Hours spent outside of class)](#section_RQ1_conclusion) </b> \n",
    "- <b> [Research Question Start (On Overall Grades)](#section_RQG) </b> \n",
    "- [Research Question - Overall Grades Graphs](#section_RQG_graphs) </b> \n",
    "- [Research Question Conclusion (On Overall Grades )](#section_RQG_conclusion) </b> \n",
    "- <b> [Research Question Start (On Confidence Level)](#section_RQC) </b> \n",
    "- [Research Question Conclusion (On Confidence Level )](#section_RQC_conclusion) </b> \n",
    "- <b> [Interview Questions](#section_interview) </b> \n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDZL97Dd_2kX"
   },
   "source": [
    "### <a id='section_sud'> Loading Data CSVs </a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "95e2tz42AIrc",
    "outputId": "57470676-0001-4780-92ef-9aadc00fe6d3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "# # drive.flush_and_unmount()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9AJm7Nd_2kj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv  \n",
    "import math\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures\n",
    "# set defaults\n",
    "plt.style.use('seaborn-white')   # seaborn custom plot style\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))   # set default size/resolution\n",
    "plt.rc('font', size=12)   # font size\n",
    "\n",
    "#importing the data files in colab\n",
    "#presurvey and demographics\n",
    "# df = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/presurvey.csv'))\n",
    "# df_d = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/CSE_Alvarado_WI20_Demographics.csv'))\n",
    "\n",
    "# df_gradesA_B = pd.read_csv(\"/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/DeID PID_CSE 8B Grades Sec A_B.csv\")\n",
    "# df_gradesC_D = pd.read_csv(\"/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/DeID PID_CSE 8B Grades Sec C_D.csv\")\n",
    "\n",
    "# df_gradesB = pd.read_csv(\"/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/DeID PID_2020-04-06T1449_Grades-CSE8B_WI20_B00.csv\")\n",
    "# df_gradesC = pd.read_csv(\"/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/DeID PID_2020-04-06T1429_Grades-CSE8B_WI20_C00.csv\")\n",
    "\n",
    "#weekly reflection\n",
    "# df_w1 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R1.csv'))\n",
    "# df_w2 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8a_winter2020_data/R2.csv'))\n",
    "# df_w3 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R3.csv'))\n",
    "# df_w4 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R4.csv'))\n",
    "# df_w5 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R5.csv'))\n",
    "# df_w6 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R6.csv'))\n",
    "# df_w7 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R7.csv'))\n",
    "# df_w8 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R8.csv'))\n",
    "# df_w9 = pd.read_csv(('/content/drive/Shared drives/CERSummerResearch/cse8b_winter2020_data/R9.csv'))\n",
    "\n",
    "#importing the data files in jupyter\n",
    "df = pd.read_csv(('presurvey.csv'))\n",
    "df_d = pd.read_csv(('CSE_Alvarado_WI20_Demographics.csv'))\n",
    "df_d = df_d.rename(columns={\"Gender(Binary)\": \"gender\"})\n",
    "\n",
    "df_gradesA_B = pd.read_csv(\"DeID PID_CSE 8B Grades Sec A_B.csv\")\n",
    "df_gradesC_D = pd.read_csv(\"DeID PID_CSE 8B Grades Sec C_D.csv\")\n",
    "df_gradesB = pd.read_csv(\"DeID PID_2020-04-06T1449_Grades-CSE8B_WI20_B00.csv\")\n",
    "df_gradesC = pd.read_csv(\"DeID PID_2020-04-06T1429_Grades-CSE8B_WI20_C00.csv\")\n",
    "df_w1 = pd.read_csv(('R1.csv'))\n",
    "df_w2 = pd.read_csv(('R2.csv'))\n",
    "df_w3 = pd.read_csv(('R3.csv'))\n",
    "df_w4 = pd.read_csv(('R4.csv'))\n",
    "df_w5 = pd.read_csv(('R5.csv'))\n",
    "df_w6 = pd.read_csv(('R6.csv'))\n",
    "df_w7 = pd.read_csv(('R7.csv'))\n",
    "df_w8 = pd.read_csv(('R8.csv'))\n",
    "df_w9 = pd.read_csv(('R9.csv'))\n",
    "list_dfs = [df_w1, df_w2, df_w3, df_w4, df_w6, df_w7, df_w8, df_w9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcXTO0Mk_2kx"
   },
   "source": [
    " Specific Thing We Will Do for CSE8B is merge the grades and figure out how many unique grades we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JpUC-MSk_2ky",
    "outputId": "ae282c14-efa1-43ad-f0b2-b88a89f98fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of grades b00 is: 147\n",
      "size of grades c00 is: 259\n",
      "size of grades A-B is: 147\n",
      "size of grades C-D is: 258\n"
     ]
    }
   ],
   "source": [
    "#We are removing last two empty rows\n",
    "# df = df.iloc[:-2]\n",
    "# print(\"The shape should be (480, 14), shape is: \" + str(df.shape))\n",
    "print(\"size of grades b00 is: \" + str(df_gradesB.shape[0]))\n",
    "print(\"size of grades c00 is: \" + str(df_gradesC.shape[0]))\n",
    "print(\"size of grades A-B is: \" + str(df_gradesA_B.shape[0]))\n",
    "print(\"size of grades C-D is: \" + str(df_gradesC_D.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Pu3P--T_2k4"
   },
   "source": [
    "### <a id='section_general_functions'> General Functions </a> You Can Use Repeatedly / Versatile to Apply to Diff Dataframes \n",
    "\n",
    "[- Return](#toc) \n",
    "- shorten_english_fluency(input_dataframe) <i> Makes the english choices answers into 1, 2, 3, 4 (in increasing level of english familiarity, respectively) </i>\n",
    "- describe_col(input_dataframe, input_column) <i> Prints out quick descriptive information about the input column </i> \n",
    "- describe_missing(input_dataframe, input_col, n) <i> Prints out example rows with missing columns </i> \n",
    "<hr>\n",
    "- convert_to_quantiative_columns(df_input) <i> finds every column that has the 1 (Not at all ) and 5 ( Completely Choices) and convert them to simple numbers 1 - 5 </i>\n",
    "- get_names(df_input, word_to_keep) *returns every column name that has the inclusion of the word* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLppB73l_2k5"
   },
   "outputs": [],
   "source": [
    "def get_names(df_input, word_to_keep = '0' ): \n",
    "    if word_to_keep == '0': \n",
    "        ans = [col for col in df_input.columns]\n",
    "    else: \n",
    "        ans =  [col for col in df_input.columns if word_to_keep in col ]\n",
    "    print(ans)\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vxBnT3A_2k8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def shorten_english_frequency(input_dataframe): \n",
    "    \"\"\"Shortening English Fluency Choices\"\"\"\n",
    "    input_dataframe.loc[(input_dataframe['english_fluency'] ==  'I learned English as a child as my primary language.'), ['english_fluency']]= 4\n",
    "    input_dataframe.loc[(input_dataframe['english_fluency'] ==  'I learned English as a child in a bilingual or multilingual setting.  I am equally comfortable in English and another language.'), ['english_fluency']]= 3\n",
    "    input_dataframe.loc[(input_dataframe['english_fluency'] ==  'I learned English after learning another primary language and I am equally comfortable in both languages.'), ['english_fluency']]= 2\n",
    "    input_dataframe.loc[(input_dataframe['english_fluency'] ==  'I learned English after learning another primary language and I am not as comfortable in English as I am in another language.'), ['english_fluency']]= 1\n",
    "    \n",
    "    input_dataframe[[\"english_fluency\"]] = input_dataframe[[\"english_fluency\"]].apply(pd.to_numeric)\n",
    "\n",
    "    return input_dataframe \n",
    "\n",
    "def describe_col(input_dataframe, input_col): \n",
    "    \"\"\"Prints quick descriptive information about the input_col\"\"\"\n",
    "#     o = (input_dataframe[input_col].value_counts(dropna = False, normalize = False, ascending = True ))\n",
    "    print(\"There are \" + str(input_dataframe[input_col].shape[0]) + \" rows in this table\")\n",
    "    print(\"There are \" + str(input_dataframe[input_col].isna().sum()) + \" empty values of \" + input_col)\n",
    "    print(\"Here are all the values (including null values) for the column \" + input_col)\n",
    "    print(input_dataframe[input_col].value_counts(dropna = False))\n",
    "    print(\"Here are all the percentages (including null values) for the column  \" + input_col)\n",
    "    print(input_dataframe[input_col].value_counts(dropna = False, normalize = True))\n",
    "    \n",
    "    if (input_dataframe[input_col].isna().sum() != 0 ):\n",
    "        print(\"Here are the percentages of the values for the column \" + input_col)\n",
    "        print(input_dataframe[input_col].value_counts(normalize = True))\n",
    "\n",
    "def describe_missing(input_dataframe, input_col, n ): \n",
    "    o = (input_dataframe[input_dataframe[input_col].isnull()])\n",
    "    if (o.shape[0] >= 1): \n",
    "        display(o.head(n))\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPOW-UvW_2lA"
   },
   "outputs": [],
   "source": [
    "def convert_to_quantiative_columns(df_input):\n",
    "    \"\"\"This function will find every column that has the 1 (Not at all ) and 5 ( Completely Choices) and convert them to simple numbers 1 - 5 \"\"\"\n",
    "    col_title = [col for col in df_input.columns if 'reflect' in col or 'i_' in col ] \n",
    "    #The reason why we are removing 'feel like i don't belong'\n",
    "    #As well as 'overall_stress' from these edited numbers is because those are \n",
    "    #Negative things while the rest of the columns are positive, so we will be replacing those\n",
    "    #Columns as 1 -> 5, 5 -> 1, and so on.\n",
    "    col_title = [col for col in col_title if 'do_belong' not in col]\n",
    "    col_title = [col for col in col_title if 'unstress_level' not in col]\n",
    "    #These are all the columns whose values we will be changing\n",
    "#     print(col_title)\n",
    "    #Looping through every column title and changing response\n",
    "    for col in col_title: \n",
    "        df_input.loc[(df_input[col] == '1 (Not at all)'), [col]]= 1\n",
    "        df_input.loc[(df_input[col] == '2'), [col]]= 2\n",
    "        df_input.loc[(df_input[col] == '3'), [col]]= 3\n",
    "        df_input.loc[(df_input[col] == '4'), [col]]= 4\n",
    "        df_input.loc[(df_input[col] == '5 (Significantly)'), [col]]= 5\n",
    "        df_input.loc[(df_input[col] == '5 (Completely)'), [col]]= 5\n",
    "        \n",
    "    \n",
    "    df_input[col_title] = df_input[col_title].apply(pd.to_numeric)\n",
    "\n",
    "    return df_input\n",
    "        \n",
    "    \n",
    "def convert_to_quantiative_columns_reverse(df_input):\n",
    "    \"\"\"This function will take columns and reverse 1 to 5, 2 to 4, 3 to 3 \"\"\"\n",
    "    col_title = [col for col in df_input.columns if 'do_belong' in col or 'unstress_level' in col ] \n",
    "    #The reason why we are editing the 'do belong' and 'unstress level' \n",
    "    #Reversed (1 = 5 and 5 = 1), is because on the survey they were phrased\n",
    "    #As \"put 5 if you feel like you do NOT belong\" and put 5 if you feel VERY stressed \n",
    "    #So for convenience of data later we've renamed columns and changed them accordingly \n",
    "  \n",
    "    #These are all the columns whose values we will be changing\n",
    "#     print(col_title)\n",
    "    \n",
    "    #Looping through every column title and changing response\n",
    "    for col in col_title: \n",
    "        df_input.loc[(df_input[col] == '1 (Not at all)'), [col]]= 5\n",
    "        df_input.loc[(df_input[col] == '2'), [col]]= 'dummy_2'\n",
    "        df_input.loc[(df_input[col] == '4'), [col]]= 2\n",
    "        df_input.loc[(df_input[col] == 'dummy_2'), [col]]= 4\n",
    "        df_input.loc[(df_input[col] == '5 (Significantly)'), [col]]= 1\n",
    "        df_input.loc[(df_input[col] == '5 (Completely)'), [col]]= 1\n",
    "    \n",
    "    df_input[col_title] = df_input[col_title].apply(pd.to_numeric)\n",
    "    return df_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YoTG8Oo_2lJ"
   },
   "source": [
    "### <a id='section_2'> Basic General Data Cleaning </a> \n",
    "\n",
    "What is happening in this section: \n",
    "- Rename Columns of Pre-Survey Data\n",
    "- Rename Columns of Weekly Classes Survey Questions\n",
    "- Combine Classes Dataframes into a Combined Dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgpBJ6cM_2lK"
   },
   "outputs": [],
   "source": [
    "#Renaming Column Names for Pre-Survey Data \n",
    "df = df.rename(columns={\"Timestamp\": \"time\",\"What.is.your.fluency.with.English.\": \"english_fluency\",'If.your.current.declared.major.'\n",
    "      + \"is.different.from.your.desired.major..i.e..you.hope.to.switch.majors...what.major.do.you.hope.to.switch.\"\n",
    "      + \"to...Leave.blank.if.this.does.not.apply.to.you..\": \"switch_major\",\"What.is.your.gender.\": \"gender\",\n",
    "           \"Did.you.enter.UCSD.as.a.transfer.student.from.another..2.year.or.4.year..college.or.university.\":\"transfer\",\n",
    "             'What.is.the.main.reason.you.are.taking.this.course.':\"main_reason\",'On.a.scale.of.1.to.5..how.much.'\n",
    "      + 'experience.did.you.have.with.programming.computer.science.before.you.took.your.FIRST.course.at.UC.San.Diego.':\n",
    "            'prior_exp','Please.select.all.of.the.CS.programming.experiences.you.had.before.taking.your.FIRST.CS.course.at.UCSD.':\n",
    "      'cs_exp','Please.rate.your.current.level.of.experience.with.technology.in.general':'tech_exp',\n",
    "      'On.a.scale.of.1.to.5..how.confident.are.you.about.your.ability.to.do.well.in.this.course.':'confidence_level',\n",
    "       'What.is.the.MINIMUM.grade.you.think.you.would.be.satisfied.with.in.this.course...' + \n",
    "      'If.you.re.taking.this.course.pass.no.pass..note.that.C..is.the.threshold.for.passing...': \"min_grade_expected\",\n",
    "      'What.is.your.ethnicity.':'ethnicity','What.is.your.race...Select.all.that.apply.':'race','DeID':'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnNHAhHc_2lR"
   },
   "outputs": [],
   "source": [
    "# rename_columns: renames the google survey questions column names to more manageable column names\n",
    "def rename_columns(df): \n",
    "    df = df.rename(columns={\"Timestamp\": \"time\", 'In.the.last.week..approximately.how.many.hours.did.you.spend.' \n",
    "        + 'outside.of.class.time.working.on.work.for.this.course.': 'hours_out_of_class',\n",
    "        'How.challenging.was.the.work.for.this.class.this.week.': 'challenge_level',\n",
    "        'In.the.past.week..which.of.the.following..if.any..did.you.seek.help.from.work.with.for.this.course.':'resource',\n",
    "        # 'In.the.past.week..which.of.the.following..if.any..did.you.seek.help.from.work.with.for.this.course.':'help',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.with.' \n",
    "        + 'your.ability.to.learn.and.complete.the.work.for.this.course..Requirements.for.other.classes.': 'i_other_classes', \n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.with.'\n",
    "        + 'your.ability.to.learn.and.complete.the.work.for.this.course..Illness.' : 'i_illness',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Family.obligations.' : 'i_family',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Work.obligations.' : 'i_work',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Social.personal.life.issues.' : 'i_personal',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Confusion.specifically.about.the.assignment.' : 'i_confusion_assignment',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Confusion.generally.about.the.material.' : 'i_confusion_material',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Getting.stuck.on.a.bug.' : 'i_confusion_bug',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Inability.to.get.help.' : 'i_help_inability',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Embarrassment.discomfort.asking.others.for.help.' : 'i_help_discomfort',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Self.doubt.lack.of.confidence.' : 'i_self_doubt',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Embarrassment.discomfort.asking.others.for.help.' : 'i_help_discomfort',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Inability.to.get.help.' : 'i_help_inability',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Lack.of.interest.in.the.assignment.or.material.' : 'i_lack_of_interest',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Inability.to.get.help.' : 'i_help_inability',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Goofing.off.procrastination.' : 'i_procrastination',\n",
    "        'In.the.past.week..on.a.scale.of.1.to.5..to.what.degree.did.each.of.the.following.interfere.'\n",
    "        + 'with.your.ability.to.learn.and.complete.the.work.for.this.course..Other..if.applicable..please.specify.below..' : 'i_other',\n",
    "        'If.you.marked..Other..above..please.specify' : 'n_if_other_custom',\n",
    "       'What.is.your.current.overall.satisfaction.with.your.performance.in.this.class.' : 'current_satisfaction', \n",
    "       'Reflecting.on.your.experiences.over.the.past.week..to.what.extent.do.you.agree.with.the.following.statements..'\n",
    "        + 'I.feel.accepted.in.this.class..' : 'reflect_accepted',\n",
    "        'Reflecting.on.your.experiences.over.the.past.week..to.what.extent.do.you.agree.with.the.following.statements..'\n",
    "        + 'I.feel.comfortable.in.this.class..' : 'reflect_comfortable',\n",
    "        'Reflecting.on.your.experiences.over.the.past.week..to.what.extent.do.you.agree.with.the.following.statements..'\n",
    "        + 'I.feel.supported.in.this.class..' : 'reflect_supported',\n",
    "        'Reflecting.on.your.experiences.over.the.past.week..to.what.extent.do.you.agree.with.the.following.statements..'\n",
    "        + 'I.feel.like.I.don.t.belong.in.this.class..' : 'reflect_do_belong',\n",
    "       'At.this.time..approximately.how.many.other.students.in.this.course.would.you.be.'\n",
    "        + 'comfortable.reaching.out.to.study.with.': 'num_study_students', \n",
    "       'Reflecting.on.the.last.week..how.stressed.have.you.been.overall.' :'reflect_unstress_level',\n",
    "       'Optional..If.you.want..please.enter.any.information.to.expand.on.or.explain.your.answers.to.any.of.the.questions.on.this.survey.' :'optional_expand_answers',\n",
    "                            \n",
    "        'Which.CSE.11.section.are.you.enrolled.in.':'cse_11_section', \n",
    "        'DeID':'id' })\n",
    "    return df \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snIUpU0X_2lV"
   },
   "outputs": [],
   "source": [
    "#renaming the weekly reflection survey data\n",
    "dfw1 = rename_columns(df_w1)\n",
    "dfw2 = rename_columns(df_w2)\n",
    "dfw3 = rename_columns(df_w3)\n",
    "dfw4 = rename_columns(df_w4)\n",
    "dfw5= rename_columns(df_w5)\n",
    "dfw6 = rename_columns(df_w6)\n",
    "dfw7 = rename_columns(df_w7)\n",
    "dfw8 = rename_columns(df_w8)\n",
    "dfw9 = rename_columns(df_w9)\n",
    "list_df = [dfw1, dfw2, dfw3, dfw4, dfw5, dfw6, dfw7, dfw8, dfw9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqKmCPNK_2lY"
   },
   "outputs": [],
   "source": [
    "def modify_columns(random_dataframe, prefix): \n",
    "    \"\"\"Edits column names by adding prefix to each of them\"\"\"\n",
    "    for col_name in random_dataframe: \n",
    "        random_dataframe = random_dataframe.rename(columns={ col_name: prefix + \"_\" + col_name}) \n",
    "    random_dataframe = random_dataframe.rename(columns={ prefix + \"_id\": \"id\"}) \n",
    "    return random_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "PKt0boLe_2lc",
    "outputId": "70348849-a145-4c3d-9372-b2196a84591b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the presurvey data is: 365\n",
      "During week 2, there were 375 students that responded\n",
      "During week 3, there were 363 students that responded\n",
      "During week 4, there were 308 students that responded\n",
      "During week 5, there were 390 students that responded\n",
      "During week 6, there were 357 students that responded\n",
      "During week 7, there were 385 students that responded\n",
      "During week 8, there were 369 students that responded\n",
      "During week 9, there were 389 students that responded\n",
      "During week 10, there were 372 students that responded\n"
     ]
    }
   ],
   "source": [
    "df1 = modify_columns(dfw1, 'wk1')\n",
    "df2 = modify_columns(dfw2, 'wk2')\n",
    "df3 = modify_columns(dfw3, 'wk3')\n",
    "df4 = modify_columns(dfw4, 'wk4')\n",
    "df5 = modify_columns(dfw5, 'wk5')\n",
    "df6 = modify_columns(dfw6, 'wk6')\n",
    "df7 = modify_columns(dfw7, 'wk7')\n",
    "df8 = modify_columns(dfw8, 'wk8')\n",
    "df9 = modify_columns(dfw9, 'wk9')\n",
    "\n",
    "list_df_combo = [df1, df2,df3,df4,df5,df6,df7,df8,df9]\n",
    "print(\"the size of the presurvey data is: \"+ str(df.shape[0]))\n",
    "for number, dataframe in enumerate( list_df_combo):\n",
    "    print(\"During week \" + str(number + 2)  + \", there were \" + str(dataframe.shape[0]) +  \" students that responded\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "jeLid8fxthZc",
    "outputId": "7bbc8d0e-af09-46f7-9f93-fd426eab9181"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nayeliguzman/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length: 462 should equal the shape: 462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_og = (df.set_index('id').join(df1.set_index('id') ,how='outer'))\n",
    "df_og = (df_og ).join(df2.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df3.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df4.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df5.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df6.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df7.set_index('id') ,how='outer')\n",
    "df_og = (df_og ).join(df8.set_index('id') ,how='outer')\n",
    "\n",
    "# df_presurvey_grades= (df.set_index('id').join(df_grades.set_index('DeID'),how='outer'))\n",
    "\n",
    "#We're shortening answer length of english fluency (4 = English is primary language, 2 = bilingual, 3 = not primary, etc)\n",
    "df = shorten_english_frequency(df)\n",
    "df_og = shorten_english_frequency(df_og)\n",
    "\n",
    "#Converted all the quantiative cols to numbers\n",
    "df_og = convert_to_quantiative_columns(df_og)\n",
    "\n",
    "#Converted all the (columns about Stress Level and Sense of Belonging) quantiative cols to numbers\n",
    "df_og = convert_to_quantiative_columns_reverse(df_og)\n",
    "\n",
    "df_og = df_og[~df_og.index.duplicated(keep='last')]\n",
    "print(\"The length: \" + str(len(df_og.index.unique())) + \" should equal the shape: \" + str(df_og.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_og.shape[0]\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPUz4fcN_2lv"
   },
   "source": [
    "##### Proceeding with df_og, A Note About Missing Survey Data \n",
    "<hr> \n",
    "\n",
    "- 365 students filled out the pre-survey data in some way (answered at least one question and received an ID) \n",
    "\n",
    "After combining all the data, \n",
    "- 462 students (unique ids) have filled out some thing at one point, whether it be a weekly reflection or survey data\n",
    "- 105 of those students did not provide info about their english fluency \n",
    "\n",
    "Going forth, we will be <b> dropping the 105 rows </b> with no English fluency information.\n",
    "\n",
    "<hr> \n",
    "\n",
    "After this, we will have \n",
    "\n",
    "- 357 rows (student ids) collected overall, of those 462 students\n",
    "    - 153 students speak english fluently only (43%) \n",
    "    - 87 students are bilingual and equally comfortable in English and another language (24.37%)\n",
    "    - 79 students learned English after learning another primary language and I am equally comfortable in both languages (22%) \n",
    "    - 38 are non-native english speakers not as comfortable with English as another language (11%)\n",
    "    \n",
    "    \n",
    "- If we split the grouping of native speakers to be speaking fluently only as well as bilingual, and non-native speakers to be learning english after primary language as well as not as comfortable with English compared to another language, \n",
    "    - 240 students we will label as native english speakers (67.22%) \n",
    "    - 117 students we will label as non-native english speakers (32.77%) \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "bZgxTRAP_2lw",
    "outputId": "e413bfa7-ff9b-4d9d-bdfe-a299eb04a879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 357 rows in this table\n",
      "There are 0 empty values of english_fluency\n",
      "Here are all the values (including null values) for the column english_fluency\n",
      "4.0    153\n",
      "3.0     87\n",
      "2.0     79\n",
      "1.0     38\n",
      "Name: english_fluency, dtype: int64\n",
      "Here are all the percentages (including null values) for the column  english_fluency\n",
      "4.0    0.428571\n",
      "3.0    0.243697\n",
      "2.0    0.221289\n",
      "1.0    0.106443\n",
      "Name: english_fluency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#By using describe_col function we see there are empty values, will drop those\n",
    "\n",
    "\n",
    "##----------------------Before dropping\n",
    "# describe_col(df_og, 'english_fluency')\n",
    "\n",
    "##----------------------After dropping \n",
    "df_og = df_og.dropna(subset=['english_fluency'])\n",
    "describe_col(df_og, 'english_fluency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLWrUqEs_2l1"
   },
   "outputs": [],
   "source": [
    "#We're creating a column where native english speakers are categorized as having chosen English as primary language\n",
    "#or bilingual and multilingual setting and equally comfortable in English\n",
    "\n",
    "df_og.native_bool = 2\n",
    "df_og.native_bool\n",
    "\n",
    "df_og['native_bool']  = (df_og['english_fluency'] == 4) \n",
    "df_og.loc[(df_og['english_fluency'] == 3), ['native_bool']]= True \n",
    "# describe_col(df_og, 'english_fluency')    \n",
    "# describe_col(df_og, 'native_bool')\n",
    "df_n = df_og.loc[df_og['native_bool']]\n",
    "df_nn = df_og.loc[df_og['native_bool'] == False]\n",
    "\n",
    "# display(df_og)\n",
    "# df_og.to_csv('file_name.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD_XMqCm_2l6"
   },
   "source": [
    "##### <a id='section_missing_data'>  Dealing With Missing Data </a> [- Return](#toc) \n",
    "\n",
    "Due to people some weeks filling out surveys and other weeks not, we will be using Data Probalistic Imputation for these \n",
    "quantitative values \n",
    "<br> \n",
    "https://afraenkel.github.io/practical-data-science/06/probabilistic-imputation.html \n",
    "<br> \n",
    "<br>\n",
    "Essentially to get rid of the empty values of when people did not fill out responses for 1-5 problems, \n",
    "we will be drawing distributions from the existing data to fill inside the missing data so our \n",
    "mean and variance will be affected as little as possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zV3HAyix_2l7"
   },
   "outputs": [],
   "source": [
    "def impute_from_distrib(df_input, col_to_impute):\n",
    "    '''use probalistic imputation on a column.'''\n",
    "    #Get the number of empty values we need to replace \n",
    "    num_null = df_input[col_to_impute].isnull().sum() # number of nulls\n",
    "    #Get a distribution with no missing data \n",
    "    fill_values = df_input[col_to_impute].dropna().sample(num_null, replace=True)  # draw fill vals from distribution\n",
    "    fill_values.index = df_input.loc[df_input[col_to_impute].isnull()].index  # align the index, which is missing?\n",
    "    imputed_df = df_input.fillna({col_to_impute: fill_values.to_dict()})  # fill the vals\n",
    "    return imputed_df \n",
    "\n",
    "\n",
    "def impute_quantitative_cols(inputing_df):\n",
    "    \"\"\"This function will take every column that asks 1-5 questions and use the impute from distr function to impute missing data \"\"\"\n",
    "    col_titles = [col for col in inputing_df.columns if '_i_' in col or '_reflect' in col \n",
    "        or '_current_satisfaction' in col or '_challenge' in col or '_num_study_students' in col]\n",
    "    for col in col_titles: \n",
    "        inputing_df = impute_from_distrib(inputing_df, col)\n",
    "    return inputing_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmVCSUUR_2mA"
   },
   "outputs": [],
   "source": [
    "#To keep missing data in df_all \n",
    "#we will now start using df_full in the process of filling missing data \n",
    "df_full = df_og.copy() \n",
    "df_full = impute_quantitative_cols(df_full)\n",
    "\n",
    "df_full = impute_from_distrib(df_full, 'wk2_hours_out_of_class')\n",
    "\n",
    "\n",
    "df_full['native_bool']  = (df_full['english_fluency'] == 4) \n",
    "df_full.loc[(df_full['english_fluency'] == 3), ['native_bool']]= True \n",
    "# describe_col(df_og, 'english_fluency')    \n",
    "# describe_col(df_og, 'native_bool')\n",
    "\n",
    "\n",
    "#We can look at the missing percentage of each column \n",
    "#And display every column who has at least some missing (null) value in at least one row \n",
    "percent_missing = df_full.isnull().sum() * 100 / len(df_full)\n",
    "missing_value_df_full = pd.DataFrame({'column_name': df_full.columns, 'percent_missing': percent_missing})\n",
    "still_missing = (missing_value_df_full[missing_value_df_full.percent_missing != 0.000000])\n",
    "# display(still_missing) \n",
    "\n",
    "# df_full.wk2_hours_out_of_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGChaOff_2mI"
   },
   "source": [
    "##### <a id='section_outlier'> How We Handle Outliers </a> [- Return](#toc) \n",
    "\n",
    "We will for certain columns such as the hours spent out of class, use our function \n",
    "- change outlier(input_df, col, lower_end, higher_end, status) *replaces outliers with median* \n",
    "\n",
    "To locate the quantiles (usually .25% and .75%) to locate the outliers of cut off points and replace them with medians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTGGORCk_2mJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_outlier(input_df, col, lower_end, higher_end, status, head = 5 ): \n",
    "    \"\"\"identifies every low part, and every high part, and replaces the stuff overlapping with median, returns column \"\"\"\n",
    "    Q1 = input_df[col].quantile(lower_end) \n",
    "    Q3 = input_df[col].quantile(higher_end) \n",
    "    IQR = Q3 - Q1 \n",
    "    cut_off_max = Q3 + 1.5 * IQR\n",
    "    cut_off_min = Q1 - 1.5 * IQR\n",
    "    median = input_df[col].median()\n",
    "    up_outliers = input_df[(input_df[col] > cut_off_max)]\n",
    "    low_outliers = input_df[(input_df[col] < cut_off_min)]\n",
    "    if status != 'replace': \n",
    "        print(\"We would be replacing \" + str(up_outliers.shape[0]) + \" rows found greater than \" \n",
    "          + str(cut_off_max) + \" with median value of \" + str(median))\n",
    "        \n",
    "        print(\"We would be replacing \" + str(low_outliers.shape[0]) + \" rows found greater than \" \n",
    "          + str(cut_off_min) + \" with median value of \" + str(median))\n",
    "        display(up_outliers[[col]].head(head))\n",
    "        display(low_outliers[[col]].head(head))\n",
    "    if status == 'replace': \n",
    "        input_df.loc[(input_df[col] > cut_off_max), [col]]= median\n",
    "        input_df.loc[(input_df[col] < cut_off_max), [col]]= median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQRMfv03_2mn"
   },
   "source": [
    "#### Concerning How We Handle Outliers of Hours Spent Outside of Course Studying: \n",
    "\n",
    "After running descriptive identifying outliers for each column, we have found that usually it seems like hours above 16 hours are unreasonable. As a result, we will be going through the columns and replacing every response that has an answer more than 15 with the median value of that column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m2AWSwDn_2mp",
    "outputId": "ca5fdc00-e1e3-4670-d823-5faa30e61e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 19 rows found greater than 25.5 with median value of 10.0\n",
      "We would be replacing 0 rows found greater than -2.5 with median value of 10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk2_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0EV6RTFG</th>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk2_hours_out_of_class\n",
       "id                              \n",
       "0EV6RTFG                    27.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk2_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk2_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 12 rows found greater than 21.0 with median value of 9.0\n",
      "We would be replacing 0 rows found greater than -3.0 with median value of 9.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk3_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23RJJJAB</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk3_hours_out_of_class\n",
       "id                              \n",
       "23RJJJAB                    40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk3_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk3_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 5 rows found greater than 31.75 with median value of 12.0\n",
      "We would be replacing 0 rows found greater than -6.25 with median value of 12.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk4_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>701C32D7</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk4_hours_out_of_class\n",
       "id                              \n",
       "701C32D7                    40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk4_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk4_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 12 rows found greater than 27.0 with median value of 10.0\n",
      "We would be replacing 0 rows found greater than -5.0 with median value of 10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk5_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23RJJJAB</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk5_hours_out_of_class\n",
       "id                              \n",
       "23RJJJAB                    40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk5_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk5_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 16 rows found greater than 25.5 with median value of 10.0\n",
      "We would be replacing 0 rows found greater than -2.5 with median value of 10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk6_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112Y19QQ</th>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk6_hours_out_of_class\n",
       "id                              \n",
       "112Y19QQ                    35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk6_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk6_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 20 rows found greater than 28.0 with median value of 12.0\n",
      "We would be replacing 0 rows found greater than -4.0 with median value of 12.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk7_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23RJJJAB</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk7_hours_out_of_class\n",
       "id                              \n",
       "23RJJJAB                    40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk7_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk7_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be replacing 17 rows found greater than 25.5 with median value of 10.0\n",
      "We would be replacing 0 rows found greater than -2.5 with median value of 10.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk8_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23RJJJAB</th>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          wk8_hours_out_of_class\n",
       "id                              \n",
       "23RJJJAB                    30.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wk8_hours_out_of_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wk8_hours_out_of_class]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'wk9_hours_out_of_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wk9_hours_out_of_class'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7d2f10e4f2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m for col_name in ['wk2_hours_out_of_class', 'wk3_hours_out_of_class', 'wk4_hours_out_of_class',\n\u001b[1;32m      3\u001b[0m                  'wk5_hours_out_of_class', 'wk6_hours_out_of_class', 'wk7_hours_out_of_class', 'wk8_hours_out_of_class', 'wk9_hours_out_of_class']:\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchange_outlier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'check'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchange_outlier_for_hours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-3dfb635c7ea1>\u001b[0m in \u001b[0;36mchange_outlier\u001b[0;34m(input_df, col, lower_end, higher_end, status, head)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchange_outlier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigher_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"identifies every low part, and every high part, and replaces the stuff overlapping with median, returns column \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mQ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigher_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mIQR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wk9_hours_out_of_class'"
     ]
    }
   ],
   "source": [
    "# If you comment out this section, it will print descriptive outlier stats\n",
    "for col_name in ['wk2_hours_out_of_class', 'wk3_hours_out_of_class', 'wk4_hours_out_of_class',\n",
    "                 'wk5_hours_out_of_class', 'wk6_hours_out_of_class', 'wk7_hours_out_of_class', 'wk8_hours_out_of_class', 'wk9_hours_out_of_class']:\n",
    "    change_outlier(df_full, col_name, 0.25, 0.75 , 'check', 1) \n",
    "\n",
    "def change_outlier_for_hours(input_df, col): \n",
    "    \"\"\"identifies every low part, and every high part, and replaces the stuff overlapping with median, returns column \"\"\"\n",
    "    median = input_df[col].median()\n",
    "    input_df.loc[(input_df[col] > 15), [col]]= median\n",
    "\n",
    "for col_name in ['wk2_hours_out_of_class', 'wk3_hours_out_of_class', 'wk4_hours_out_of_class',\n",
    "                 'wk5_hours_out_of_class', 'wk6_hours_out_of_class', 'wk7_hours_out_of_class', 'wk8_hours_out_of_class', 'wk9_hours_out_of_class']:\n",
    "    change_outlier_for_hours(df_full, col_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeOernFu_2oM"
   },
   "outputs": [],
   "source": [
    "df_native = df_full.loc[df_full['native_bool']]\n",
    "df_non_native = df_full.loc[df_full['native_bool'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFzXKUrX_2oO"
   },
   "source": [
    "### <a id='section_dfs'> List of Main Dataframes </a> \n",
    "\n",
    "---\n",
    "[- Return](#toc) \n",
    "\n",
    "These will be the dataframes we are calling\n",
    "- df = Pre-survey data\n",
    "- df_og = Pre-survey data combined with responses from weeks 2-9, leaving all missing data as well as outliers\n",
    "- df_full = Pre-survey data combined with respones from 2-9, with data imputed as well as outliers replaced with medians \n",
    "- df_presurvey_grades = Combination of Pre-survey data and grades data \n",
    "- df_n = part of df_og that only contains responses from native english speakers \n",
    "- df_nn = part of df_og that only contains responses from non-native english speakers\n",
    "- df_native = part of df_full that only contains responses from native english speakers, imputed data \n",
    "- df_non_native = part of df_full that only contains responses from non-native english speakers, imputed data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jbg20aJR_2oP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yZ8qvqHH_2oT"
   },
   "source": [
    "### <a id='section_funcs_mean'> General Mean Functions </a> [- Return](#toc) \n",
    "These will be usable functions for getting means of columns and editing them\n",
    "- get_single_mean(df_input, col) <i> Gets mean values for just one specific column </i> \n",
    "- get_mean(df_input, col_list) *returns mean of every single column name in the column list parameter*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-EKRZH-_2oU"
   },
   "outputs": [],
   "source": [
    "def get_single_mean(df_input, col):\n",
    "    '''get and return the mean values for this column.'''\n",
    "    #to numeric converts strings to integer values, coerce replaces non-numeric with nan\n",
    "    mean = pd.to_numeric(df_input[col], errors='coerce').mean()\n",
    "    print(\"The mean for \" + col + \" is \" + str(mean))\n",
    "    return mean \n",
    "\n",
    "def get_mean(df_input, col_list):\n",
    "    '''get and return the mean values for this column.'''\n",
    "    answers = []\n",
    "    #to numeric converts strings to integer values, coerce replaces non-numeric with nan\n",
    "    for col in col_list: \n",
    "        mean = pd.to_numeric(df_input[col], errors='coerce').mean()\n",
    "#         print(\"The mean for \" + col + \" is \" + str(mean))\n",
    "        answers.append(mean)\n",
    "    return answers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZnIumz7nkZy"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWGZi48R7fE-"
   },
   "source": [
    "# General Get Dataframe Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9XhJIPd6jYD"
   },
   "outputs": [],
   "source": [
    "def get_df_n(df, print_status = False ): \n",
    "  new_dataframe = df[df['native_bool'] == True]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" native students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_nn(df, print_status = False ): \n",
    "  new_dataframe = df[df['native_bool'] == False]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" non-native students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_male(df, print_status = False ): \n",
    "  new_dataframe = df[df['gender'] == 'Male']\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" male students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_female(df, print_status = False ): \n",
    "  new_dataframe = df[df['gender'] == 'Female']\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" female students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_n_female(df, print_status = False ): \n",
    "  new_dataframe = df[(df['native_bool'] == True) & (df['gender'] == 'Female')]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" native female students\")\n",
    "  return new_dataframe\n",
    "  \n",
    "def get_df_n_male(df, print_status = False ): \n",
    "  new_dataframe = df[(df['native_bool'] == True) & (df['gender'] == 'Male')]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" native male students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_nn_female(df, print_status = False ): \n",
    "  new_dataframe = df[(df['native_bool'] == False) & (df['gender'] == 'Female')]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" non-native female students\")\n",
    "  return new_dataframe\n",
    "\n",
    "def get_df_nn_male(df, print_status = False ): \n",
    "  new_dataframe = df[(df['native_bool'] == False) & (df['gender'] == 'Male')]\n",
    "  if print_status:\n",
    "    print(str(new_dataframe.shape[0]) + \" non-native male students\")\n",
    "  return new_dataframe\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGEaAJy_kAnb"
   },
   "source": [
    "# General Statistical Test Functions\n",
    "\n",
    "These will be usable functions for statistical testing\n",
    "\n",
    "- run_t_test(list_from_sample_1, list_from_sample_2, alpha) Runs a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.\n",
    "- run_welch_test(list_from_sample_1, list_from_sample_2, equal_var = False, describe = True) Runs a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test DOES NOT assume that the populations have identical variances by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSldzbBCY8FA"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "from scipy.stats import t\n",
    " \n",
    "# function for calculating the t-test for two independent samples\n",
    "def independent_ttest(data1, data2):\n",
    "\t# calculate means\n",
    "\tmean1, mean2 = np.mean(data1), np.mean(data2)\n",
    "\t# calculate standard errors\n",
    "\tse1, se2 = sem(data1), sem(data2)\n",
    "\t# standard error on the difference between the samples\n",
    "\tsed = math.sqrt(se1**2.0 + se2**2.0)\n",
    "\t# calculate the t statistic\n",
    "\tt_stat = (mean1 - mean2) / sed\n",
    "\t# degrees of freedom\n",
    "\tdf = len(data1) + len(data2) - 2\n",
    "\t# calculate the p-value\n",
    "\tp = (1.0 - t.cdf(abs(t_stat), df)) * 2.0\n",
    "\t# return everything\n",
    "\treturn t_stat, df, p\n",
    "\n",
    "def run_t_test(one, two ):\n",
    "  # calculate the t test\n",
    "\t\n",
    "  t_stat, df, p = independent_ttest(one, two )\n",
    "\t\n",
    "  print('T-test=%.3f, df=%d, p=%.3f' % (t_stat, df, p))\n",
    "  # interpret via p-value\n",
    "  if p > 0.05:\n",
    "    print('Decision: Fail to Reject the null hypothesis')\n",
    "  else:\n",
    "    print('Decision: Reject the null hypothesis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Mu_AL6i2fu"
   },
   "outputs": [],
   "source": [
    "#Welch's t-test\n",
    "def run_welch_test(a,b, equal_var=False,describe = True ):\n",
    "    #Calculate degrees of freedom\n",
    "    dof_num = (a.var()/a.size + b.var()/b.size)**2\n",
    "    dof_den =(a.var()/a.size)**2 / (a.size-1) + (b.var()/b.size)**2 / (b.size-1)\n",
    "    dof = dof_num/dof_den\n",
    "    #Perform Welch's t-test\n",
    "    test, p_val = ttest_ind(a,b, equal_var = equal_var)\n",
    "    if describe:\n",
    "        print(f\"Welch's t-test={test}, df={dof}, p-value={p_val}\")\n",
    "        #Print decision.\n",
    "        if p_val<0.05:\n",
    "            print('Decision: Reject the null hypothesis')\n",
    "        else:\n",
    "            print('Decision: Fail to reject the null hypothesis')\n",
    "    return ttest_ind(a,b, equal_var = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statistics \n",
    "\n",
    "#Mann-Whitney U test\n",
    "def run_Mann_Whitney_test(NNES, NES):\n",
    "    #Calculate the median\n",
    "    NNES_mdn = statistics.median(NNES)\n",
    "    NES_mdn = statistics.median(NES)\n",
    "    print('NNES median: ' + str(NNES_mdn))\n",
    "    print('NES median: ' + str(NES_mdn))\n",
    " \n",
    "    #Perform Mann Whitney U Test\n",
    "    u_statistic, p_val = mannwhitneyu(NNES, NES,use_continuity = True)\n",
    "    print(f\"Mann Whitney U-test={u_statistic}, p-value={p_val}\")\n",
    "      #Print decision\n",
    "    if p_val<0.05:\n",
    "        print('Decision: Reject the null hypothesis')\n",
    "    else:\n",
    "        print('Decision: Fail to reject the null hypothesis')\n",
    "    return mannwhitneyu(NNES, NES, use_continuity = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import levene\n",
    "\n",
    "def run_Levene_test(df1, df2):\n",
    "    stat, p = levene(df1, df2, center=\"mean\")\n",
    "    if (p < 0.05) == True:\n",
    "      print('Reject the null hypothesis that the variances are equal.')\n",
    "      print(\"the p value is: \" + str(p) + \" < 0.05\")\n",
    "      print(\"A significant result indicates that the variances are not equal and would \")\n",
    "      print(\"be a violation of the assumption of equal variances.\")\n",
    "\n",
    "    else:\n",
    "      print('Cannot reject the null hypothesis that the variances are equal.')\n",
    "      print(\"The null hypothesis: the variances across both nn and ne groups are equal for confidence level\")\n",
    "      print(\"the p value is: \" + str(p) + \" > 0.05\")\n",
    "      print(\"If this test was for ANCOVA, you may proceed to check other assumptions.\")\n",
    "\n",
    "    print(dfa_nn['confidence_level'].var())\n",
    "    dfa_ne['confidence_level'].var()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcR3ZLLm_2oY"
   },
   "source": [
    "# Research Question Hours\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruzd-IFR9eQr"
   },
   "source": [
    "## Q: Is there a difference in the amount of time spent outside of class (for readings, assignments etc) in a CS course between native and non-native English speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "ocUwwjQ-8QGf",
    "outputId": "ca834bfe-b3e2-4653-9b08-27aea546a653"
   },
   "outputs": [],
   "source": [
    "def impute_hours(inputing_df):\n",
    "    col_titles = [col for col in inputing_df.columns if 'hours_out_of_class' in col]\n",
    "    for col in col_titles: \n",
    "        inputing_df = impute_from_distrib(inputing_df, col)\n",
    "    return inputing_df\n",
    "\n",
    "\n",
    "def change_outlier_for_hours(input_df, col): \n",
    "    \"\"\"identifies every low part, and every high part, and replaces the stuff overlapping with median, returns column \"\"\"\n",
    "    median = input_df[col].median()\n",
    "    input_df.loc[(input_df[col] > 20), [col]]= median\n",
    "    return input_df\n",
    "\n",
    "hours_list = get_names(df_full, 'hours_out_of_class')\n",
    "df_hours_without_outliers = df_full.copy()\n",
    "\n",
    "for col_name in ['wk2_hours_out_of_class', 'wk3_hours_out_of_class', 'wk4_hours_out_of_class',\n",
    "                 'wk5_hours_out_of_class', 'wk6_hours_out_of_class', 'wk7_hours_out_of_class', 'wk8_hours_out_of_class', 'wk9_hours_out_of_class']:\n",
    "    df_hours_without_outliers = change_outlier_for_hours(df_hours_without_outliers, col_name) \n",
    "\n",
    "\n",
    "df_hr = df_hours_without_outliers.copy()\n",
    "df_hours_without_outliers = df_hours_without_outliers[['english_fluency', 'native_bool', 'wk2_hours_out_of_class',\n",
    "                    'wk3_hours_out_of_class', 'wk4_hours_out_of_class', 'wk5_hours_out_of_class',\n",
    "                    'wk6_hours_out_of_class', 'wk7_hours_out_of_class', 'wk8_hours_out_of_class', 'wk9_hours_out_of_class']]\n",
    "\n",
    "#comment and uncomment this to impute or not impute the data\n",
    "df_hours_without_outliers = impute_hours(df_hours_without_outliers)\n",
    "df_hours = df_hours_without_outliers.copy()\n",
    "\n",
    "# describe_col(df_hours_without_outliers_imputed, 'wk3_hours_out_of_class')\n",
    "\n",
    "df_hours_n = df_hours[df_hours['native_bool'] == True ]\n",
    "df_hours_nn = df_hours[df_hours['native_bool'] == False ]\n",
    "overall_list_hours_n = []\n",
    "overall_list_hours_nn = []\n",
    "\n",
    "for col in hours_list: \n",
    "    overall_list_hours_n.append(df_hours_n[col].tolist())\n",
    "\n",
    "for col in hours_list: \n",
    "    overall_list_hours_nn.append(df_hours_nn[col].tolist())\n",
    "\n",
    "overall_list_hours_n = [y for x in overall_list_hours_n for y in x]\n",
    "overall_list_hours_nn = [y for x in overall_list_hours_nn for y in x]\n",
    "\n",
    "\n",
    "print(\"For \" + str(len(overall_list_hours_n)) + \" inputs from native english speaking students, \"\n",
    "  + \"their mean hours studied was \" +  str(np.mean(overall_list_hours_n)))\n",
    "print(\"For \" + str(len(overall_list_hours_nn)) + \" inputs from non-native english speaking students, \"\n",
    "  + \"their mean hours studied was \" +  str(np.mean(overall_list_hours_nn)))\n",
    "\n",
    "# print(\"--------------------\")\n",
    "print(ttest_ind(overall_list_hours_n,overall_list_hours_nn, nan_policy = 'omit'))\n",
    "print(ttest_ind(overall_list_hours_n,overall_list_hours_nn,  equal_var = False, nan_policy = 'omit'))\n",
    "\n",
    "print(\"--------------------\")\n",
    "# run_t_test(overall_list_hours_n, overall_list_hours_nn)\n",
    "# run_welch_test(overall_list_hours_n, overall_list_hours_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bnsyu6DW_2oq"
   },
   "outputs": [],
   "source": [
    "def make_simple_mean_df(df, col_input = 'wk2_hours_out_of_class', group_input = 'native_bool' ):\n",
    "    df_just_other_classes = df[[col_input, group_input]]\n",
    "    new_name = \"avg_of_\" + col_input\n",
    "    df_just_other_classes = df_just_other_classes.rename(columns={group_input: \"native_bool\", col_input: new_name})\n",
    "    df_just_other_classes = df_just_other_classes.reset_index(drop=True)\n",
    "    means_table = df_just_other_classes.groupby('native_bool').mean()\n",
    "    display(means_table)\n",
    "    \n",
    "    var_table = df_just_other_classes.groupby('native_bool').var()\n",
    "    display(var_table)\n",
    "    return means_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLDolN47_2oz"
   },
   "source": [
    "#### Null Hypothesis:\n",
    "The average hours of class spent outside studying for non-native english speakers is equal to the average hours of class spent outside studying for native english speakers.\n",
    "#### Alternative Hypothesis: \n",
    "The average hours are different\n",
    "\n",
    "#### Type of Statistical Analysis: \n",
    "##### Independent Samples T Test\n",
    "\n",
    "- Assumption of Independence: you need two independent, categorical groups that represent your independent variable. Yes\n",
    "- Assumption of normality: the dependent variable should be approximately normally distributed. The dependent variable should also be measured on a continuous scale.\n",
    "- Assumption of Homogeneity of Variance: The variances of the dependent variable should be equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XdvmSTF_2o7"
   },
   "source": [
    "### <a id='section_RQ1_graphs'> Research Question Graphs #1 </a>   [- Return](#toc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "nZBLqbP1_2o7",
    "outputId": "2bef4655-bfb5-47af-b29f-54c67269fad9"
   },
   "outputs": [],
   "source": [
    "#Checking Normality of dependent variable (hours out of class spent studying for class)\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(overall_list_hours_n)\n",
    "pyplot.hist(overall_list_hours_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RXjCPtsu_2pA",
    "outputId": "a5214f36-ba0f-4f94-cfc7-fe77078f15ca"
   },
   "outputs": [],
   "source": [
    "df_hours_all = df_hours\n",
    "keys = [c for c in df_hours_all if c.startswith('wk')]\n",
    "df_hours_all = pd.melt(df_hours_all, id_vars='native_bool', value_vars=keys, value_name='hours')\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk2_hours_out_of_class' ), ['variable']]= 2\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk3_hours_out_of_class' ), ['variable']]= 3\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk4_hours_out_of_class' ), ['variable']]= 4\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk5_hours_out_of_class' ), ['variable']]= 5\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk6_hours_out_of_class' ), ['variable']]= 6\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk7_hours_out_of_class' ), ['variable']]= 7\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk8_hours_out_of_class' ), ['variable']]= 8\n",
    "df_hours_all.loc[(df_hours_all['variable'] == 'wk9_hours_out_of_class' ), ['variable']]= 9\n",
    "df_hours_all = df_hours_all.rename(columns={\"variable\": \"week\"})\n",
    "df_hours_all.columns\n",
    "\n",
    "graph_1hour = sns.catplot(x=\"native_bool\", y=\"hours\", hue = 'week', kind=\"point\", data= df_hours_all);\n",
    "graph_1hour.set(ylim=(0, None))\n",
    "print(graph_1hour)\n",
    "\n",
    "sns.catplot(x= 'week', y=\"hours\", hue=\"native_bool\", kind=\"box\", data= df_hours_all);\n",
    "# sns.catplot(x=\"week\", y=\"hours\", hue=\"native_bool\",kind=\"violin\", data= df_hours_all);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6F9tAl6_2pE"
   },
   "outputs": [],
   "source": [
    "# df_hours_n.wk2_hours_out_of_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "1czwBR0D_2pH",
    "outputId": "7e5f9fd0-1835-424c-a50b-ecff154bdc39"
   },
   "outputs": [],
   "source": [
    "for col in hours_list:\n",
    "    col_test = col\n",
    "    print(\"Independent T-Test for Column \" + col + \" is \" + str(ttest_ind(df_hours_n[col_test], df_hours_nn[col_test], equal_var=False)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "75flLyO6_2pJ",
    "outputId": "4c15e52f-b0f7-4552-c6e0-d6f07590d4be"
   },
   "outputs": [],
   "source": [
    "ttest_ind(overall_list_hours_nn, overall_list_hours_n, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgf7xfSG_2pL"
   },
   "source": [
    "### <a id='section_RQ1_conclusion'> Research Question Conclusion #1 </a>   [- Return](#toc) \n",
    "##### Is there a difference in the amount of time spent outside of class (for readings, assignments etc) in a CS course between native and non-native English speakers?\n",
    "\n",
    "Technical Notes:\n",
    "- used ttest_ind underestimates p for unequal variances\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "- histogram does not show very normal distribution? \n",
    "\n",
    "We ran a T-test using for the means of two independent samples of scores and with a p value of 4.03e-07, we *reject* the null hypothesis in that overall there \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EG7opqkBzvM"
   },
   "source": [
    "## A: Yes, the average hours spent studying for non-native speakers are statistically higher than native english speakers. \n",
    "\n",
    "\n",
    "Statistically different: Native students spend about 5.5 hrs on average, non-native about 6.1 hrs on average across all 9 weeks of CSE 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ks9XRE7ChBSX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJ4VwA79_2pM"
   },
   "source": [
    "# <a id='section_RQG'> Research Question Grades </a> \n",
    "  [- Return](#toc) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wmLpH_cIG31"
   },
   "source": [
    "### Q: Is there a difference in the overall grade received by non-native english speaker students and native english speaker students? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "h7Id8uCLNBiJ",
    "outputId": "bea50999-3348-4dac-f9f6-5b78fbcd325d"
   },
   "outputs": [],
   "source": [
    "df_grades = pd.read_csv(os.path.join('/content/drive/Shared drives/CERSummerResearch/data/DeID GradeReporting_CSE11_F19.csv'))\n",
    "print(str(df_grades.shape[0]) + \" students received grades in the class.\")\n",
    "print(str((df_full.shape[0]/df_grades.shape[0])*100) + \" % of students filled out the pre-survey.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "b7d1doeT_2pM",
    "outputId": "d28136e9-5f82-406f-e32e-9bfa7d64194e"
   },
   "outputs": [],
   "source": [
    "df_grades_mini = df_grades[['Overall.weighted.course.percentage', 'Final.exam.percentage', 'DeID']]\n",
    "dfg = (df.set_index('id')).join(df_grades_mini.set_index('DeID'))\n",
    "dfg = dfg.rename(columns={\"Overall.weighted.course.percentage\": \"overall_grades\", \"Final.exam.percentage\": \"final_exam\"})\n",
    "dfg['native_bool']  = (dfg['english_fluency'] == 4) \n",
    "dfg.loc[(dfg['english_fluency'] == 3), ['native_bool']]= True \n",
    "dfg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQiK_vj9_2pP"
   },
   "source": [
    "#### <a id='section_RQG_graphs'> Research Question Graphs On Overall Grades </a>\n",
    "\n",
    "   [- Return](#toc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ztrVliqN_2pR",
    "outputId": "514060cd-4c8d-46aa-d5f0-b66c3b2096a1"
   },
   "outputs": [],
   "source": [
    "graph_1 = sns.catplot(x=\"native_bool\", y=\"overall_grades\", kind=\"box\", data=dfg, );\n",
    "graph_1.set(ylim=(0, None))\n",
    "print(graph_1)\n",
    "graph_2 = sns.catplot(x=\"native_bool\", y=\"overall_grades\", hue = \"confidence_level\", kind=\"box\", data=dfg);\n",
    "graph_2.set(ylim=(0, None))\n",
    "print(graph_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBC_LjGgCouo"
   },
   "outputs": [],
   "source": [
    "# graph_2 = sns.distplot(x=\"native_bool\", y=\"overall_grades\", hue = \"confidence_level\", kind=\"box\", data=dfg);\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "xPtGBrfn_2pe",
    "outputId": "293f1204-3db9-42fa-8f82-539592fbe2e8"
   },
   "outputs": [],
   "source": [
    "dfg_imputed = impute_from_distrib(dfg, 'overall_grades')\n",
    "dfg_n = dfg[dfg['native_bool'] == True]\n",
    "dfg_nn = dfg[dfg['native_bool'] == False]\n",
    "\n",
    "dfg_imputed_n = dfg_imputed[dfg_imputed['native_bool'] == True]\n",
    "dfg_imputed_nn = dfg_imputed[dfg_imputed['native_bool'] == False]\n",
    "\n",
    "\n",
    "get_single_mean(dfg_n, 'overall_grades')\n",
    "get_single_mean(dfg_nn, 'overall_grades')\n",
    "print(\"The variance for native speaker overall grade score is \" + str(np.var(dfg_n.overall_grades)))\n",
    "print(\"The variance for non-native speaker overall grade score is \" + str(np.var(dfg_nn.overall_grades)))\n",
    "ttest_ind(dfg_n['overall_grades'], dfg_nn['overall_grades'], equal_var=False)\n",
    "print(ttest_ind(dfg_n['overall_grades'], dfg_nn['overall_grades'], equal_var=False, nan_policy = 'omit'))\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------RESULTS WITH IMPUTED DATA-----------------\")\n",
    "print(\"\")\n",
    "get_single_mean(dfg_imputed_n, 'overall_grades')\n",
    "get_single_mean(dfg_imputed_nn, 'overall_grades')\n",
    "print(\"The variance for native speaker overall grade score is \" + str(np.var(dfg_imputed_n.overall_grades)))\n",
    "print(\"The variance for non-native speaker overall grade score is \" + str(np.var(dfg_imputed_nn.overall_grades)))\n",
    "print(ttest_ind(dfg_imputed_n['overall_grades'], dfg_imputed_nn['overall_grades'], equal_var=False))\n",
    "print(ttest_ind(dfg_imputed_n['overall_grades'], dfg_imputed_nn['overall_grades'], equal_var=False, nan_policy = 'omit'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "4R_sxlUwZwTC",
    "outputId": "658556e1-2735-4ddb-c381-dff0a3573718"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Method 1: on the same Axis\n",
    "a = sns.distplot( dfg_n.overall_grades , bins = 50,  color=\"skyblue\", label=\"Native English Speakers\", )\n",
    "a = sns.distplot( dfg_nn.overall_grades , bins = 50, color=\"red\", label=\"Non-Native English Speakers\")\n",
    "a.set_title('Distribution of Overall Grades For Native VS Non-Native English Speakers')\n",
    "a.set_ylabel('Number of Students')\n",
    "a.set_xlabel('Overall Grade (%)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "JV6XHNkXQoWG",
    "outputId": "60cb0959-c16b-4c0f-df36-267d5e3723ea"
   },
   "outputs": [],
   "source": [
    "print(\"Mean for I learned English as a child as my primary language overall grade is \" + str(dfg[dfg['english_fluency'] == 4 ].overall_grades.mean()))\n",
    "print(\"Mean for bilingual or multilingual setting.  I am equally comfortable in English and another language. overall grade is \" + str(dfg[dfg['english_fluency'] == 3 ].overall_grades.mean()))\n",
    "print(\"Mean another primary language and I am equally comfortable in both languages average overall grade is \" + str(dfg[dfg['english_fluency'] == 2 ].overall_grades.mean()))\n",
    "print(\"Mean another primary language and I am not as comfortable in English as I am in another language. overall grade is \" + str(dfg[dfg['english_fluency'] == 1 ].overall_grades.mean()))\n",
    "\n",
    "dfg.corr(method ='pearson') \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uFkYGg8ljqck",
    "outputId": "c610b93e-a733-42d0-ccbc-0c3a15b04a2b"
   },
   "outputs": [],
   "source": [
    "# #Welch's t-test\n",
    "# def welch_t(a,b, equal_var=False,describe = True ):\n",
    "#     #Calculate degrees of freedom\n",
    "#     dof_num = (a.var()/a.size + b.var()/b.size)**2\n",
    "#     dof_den =(a.var()/a.size)**2 / (a.size-1) + (b.var()/b.size)**2 / (b.size-1)\n",
    "#     dof = dof_num/dof_den\n",
    "#     #Perform Welch's t-test\n",
    "#     test, p_val = ttest_ind(a,b, equal_var = equal_var)\n",
    "#     if describe:\n",
    "#         print(f\"Welch's t-test = {test}\\np-value: {p_val}\\nDoF: {dof}\")\n",
    "#         #Print decision.\n",
    "#         if p_val<0.05:\n",
    "#             print('Decision: Reject the null hypothesis')\n",
    "#         else:\n",
    "#             print('Decision: Fail to reject the null hypothesis')\n",
    "#     return ttest_ind(a,b, equal_var = False)\n",
    "\n",
    "welch1 = run_welch_test(dfg_imputed_n.overall_grades, dfg_imputed_nn.overall_grades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HI4Aqfp2_2pj"
   },
   "source": [
    "## A: Yes, non-native students perform significantly higher than native students\n",
    "[- Return](#toc) \n",
    "\n",
    "After running the t-test and getting a p-value less than 0.05, we reject the null hypothesis of identical average scores of non-native english speaker students and native english speaker students "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLk4qUW2nIYS"
   },
   "source": [
    "# Research Question Confidence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAEV0QhHnK6l"
   },
   "source": [
    "### Q: Are the starting confidence level of native english speakers statistically different from non-native english speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPKx2yLcUNIB",
    "outputId": "1b443a46-361c-4a42-baab-12439f60388d"
   },
   "outputs": [],
   "source": [
    "dfr = df_og.copy() \n",
    "# dfr = dfr[dfr['prior_exp'] == 3\n",
    "dfr['native_bool']  = (dfr['english_fluency'] == 4) \n",
    "dfr.loc[(dfr['english_fluency'] == 3), ['native_bool']]= True \n",
    "y = 'confidence_level'\n",
    "\n",
    "# get_names(df_w9)\n",
    "# dfr = convert_wk9_col_to_quantitative(dfr, [y])\n",
    "\n",
    "#Comment and Uncomment to see differences if rather than dropping na, you imputed data\n",
    "# dfr = impute_from_distrib(dfr, y)\n",
    "print(\"Originally, there are \" + str(dfr[y].isna().sum()) + \" empty values for \" + y + \" overall\") \n",
    "dfr=dfr.dropna(subset=[y])\n",
    "\n",
    "print(\"Dropping empty falues, there are now \" + str(dfr[y].isna().sum()) + \" empty values for \" + y + \" overall\") \n",
    "print(\"Printing out means for \" + y) \n",
    "print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_n(dfr, True), y), 4)) + \"\") \n",
    "print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_nn(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_n_female(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_nn_female(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_n_male(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_nn_male(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_female(dfr, True), y), 4))+ \"\") \n",
    "# print(\" average \" + y + \" \" + str(round(get_single_mean(get_df_male(dfr, True), y), 4))+ \"\") \n",
    "import scipy.stats as st\n",
    "\n",
    "dfr_n = get_df_n(dfr)\n",
    "dfr_nn = get_df_nn(dfr)\n",
    "# dfr_f = get_df_female(dfr)\n",
    "# dfr_m = get_df_male(dfr)\n",
    "# dfr_n_female = get_df_n_female(dfr)\n",
    "# dfr_n_male = get_df_n_male(dfr)\n",
    "# dfr_nn_female = get_df_nn_female(dfr)\n",
    "# dfr_nn_male = get_df_nn_male(dfr)\n",
    " \n",
    "# data_dfr = {'Native':[get_single_mean(dfr_n, y),get_single_mean(dfr_n_male, y),get_single_mean(dfr_n_female, y),], \n",
    "#         'Non-Native':[get_single_mean(dfr_nn, y),get_single_mean(dfr_nn_male, y),get_single_mean(dfr_nn_female, y),],\n",
    "#         'N & Non-N':[get_single_mean(dfr, y),get_single_mean(dfr_m, y),get_single_mean(dfr_f, y),],\n",
    "#         'Type':['Male & Female', 'Male', 'Female']\n",
    "#         } \n",
    "# dfr_special = pd.DataFrame(data_dfr).set_index('Type')\n",
    "# display(dfr_special)\n",
    "\n",
    "print(st.levene( dfr_n[y], dfr_nn[y]))\n",
    "print(\"P value greater than 0.05, can run T-Test\")\n",
    "# print(\"P value less than 0.05, run Welch Test equal_var = False\")\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"-----T-Test For Native & Non-Native---------------------\")\n",
    "# run_t_test(dfr_n[y], dfr_nn[y])\n",
    "# print(\"\")\n",
    "# print(\"--------T-Test For Male & Female ---------------------\")\n",
    "# run_t_test(dfr_m[y], dfr_f[y])\n",
    "# print(\"\")\n",
    "# print(\"-----T-Test For Native Male & Native Female---------------------\")\n",
    "# run_t_test(dfr_n_male[y], dfr_n_female[y])\n",
    "# print(\"\")\n",
    "# print(\"-----T-Test For Non-Native Male & Non-Native Female---------------------\")\n",
    "# run_t_test(dfr_nn_male[y], dfr_nn_female[y])\n",
    "# print(\"\")\n",
    "# print(\"-----T-Test For Native Male & Non-Native Male---------------------\")\n",
    "# run_t_test(dfr_n_male[y], dfr_nn_male[y])\n",
    "# print(\"\")\n",
    "# print(\"-----T-Test For Native Female & Non-Native Female---------------------\")\n",
    "# run_t_test(dfr_n_female[y], dfr_nn_female[y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results to not change for imputed vs non imputed data.\n",
    "There are 6 empty rows for confidence level so we will impute these rows when running analysis.\n",
    "Then we will drop these rows to check if the result is the same with non-imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['native_bool']  = (df['english_fluency'] == 4) \n",
    "df.loc[(df['english_fluency'] == 3), ['native_bool']]= True \n",
    "dfc = df.copy()\n",
    "dfc_full = dfc.copy()     \n",
    "\n",
    "#-----------Check if result changes with imputation\n",
    "dfc_full = impute_from_distrib(dfc_full, 'confidence_level')\n",
    "describe_col(dfc_full,'confidence_level')\n",
    "\n",
    "\n",
    "# #-----------Check if result changes with dropping empty rows\n",
    "# dfc_full = dfc_full.dropna(subset=['confidence_level'])\n",
    "# describe_col(dfc_full,'confidence_level')\n",
    "\n",
    "#getting the population size of native and non native English speakers\n",
    "dfc_n = dfc_full[dfc_full['native_bool'] == True]\n",
    "dfc_nn = dfc_full[dfc_full['native_bool'] == False]\n",
    "print(\"The population size of NES is \" + str(dfc_n.shape[0]))\n",
    "print(\"The population size of NNES is \" + str(dfc_nn.shape[0]))\n",
    "\n",
    "#getting the mean confidence \n",
    "mean_confidence_native = get_single_mean(dfc_full[dfc_full['native_bool'] == True], 'confidence_level')\n",
    "mean_confidence_non_native = get_single_mean(dfc_full[dfc_full['native_bool'] == False], 'confidence_level')\n",
    "print(\"The mean confidence level for native english speakers is \" + str(mean_confidence_native))\n",
    "print(\"The mean confidence level for non-native english speakers is \" + str(mean_confidence_non_native))\n",
    "\n",
    "#printing the tests\n",
    "print(\"---------------------T-Test---------------------\")\n",
    "run_t_test(dfc_full[dfc_full['native_bool'] == True].confidence_level, dfc_full[dfc_full['native_bool'] == False].confidence_level)\n",
    "print(\"---------------------Welch Test-------------------\")\n",
    "run_welch_test(dfc_full[dfc_full['native_bool'] == True].confidence_level, dfc_full[dfc_full['native_bool'] == False].confidence_level)\n",
    "print('----------------Mann-Whitney U test---------------')\n",
    "run_Mann_Whitney_test(dfc_full[dfc_full['native_bool'] == True].confidence_level,dfc_full[dfc_full['native_bool'] == False].confidence_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of imputation the results of these tests do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double checking the t-test!\n",
    "df_new = dfc_full.copy()\n",
    "df_confidence_n = df_new [df_new ['native_bool'] == True ]\n",
    "df_confidence_nn = df_new [df_new ['native_bool'] == False ]\n",
    "\n",
    "result = ttest_ind(df_confidence_n['confidence_level'],df_confidence_nn['confidence_level'],equal_var=False)\n",
    "                \n",
    "if (result.pvalue < 0.05):\n",
    "  print(\"Reject the null hypothesis that the means of \" + y + \" are equal for native and non-native.\")\n",
    "  print(str(result.pvalue) + \"< 0.05\")\n",
    "  print('T-test results for average ' + y + ' between native and non-native:')\n",
    "  print(str(result))\n",
    "else:\n",
    "  print(\"We cannot reject the null hypotheis that the means for \" + y + \" are equal for native and non-native\")\n",
    "  print(str(result.pvalue) + '> 0.05')\n",
    "  print('T-test results for average ' + y + ' between native and non-native:')\n",
    "  print(str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFxg3Z2coIt_"
   },
   "outputs": [],
   "source": [
    "\n",
    "dfgc = dfc_full.copy(deep = True) \n",
    "\n",
    "df_confidence_n = dfgc[dfgc['native_bool'] == True ]\n",
    "\n",
    "df_confidence_nn = dfgc[dfgc['native_bool'] == False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "i0IM__Q7q2dC",
    "outputId": "ae94a819-5988-42a5-db88-b837ddd8bed3"
   },
   "outputs": [],
   "source": [
    "df_confidence = dfgc[['native_bool', 'confidence_level']]\n",
    "print(\"Values for native english speakers\")\n",
    "print(df_confidence_n['confidence_level'].value_counts(normalize = True ).sort_index())\n",
    "print(\"Values for non-native english speakers\")\n",
    "print(df_confidence_nn['confidence_level'].value_counts(normalize = True ).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "Oi6GyUghB7Jj",
    "outputId": "f467cfa0-1696-49b1-a5e0-4b1c96f192ad"
   },
   "outputs": [],
   "source": [
    "confidence_data = {'1 (Not at all confident)':[0.020649, 0.028369], '2 ':[0.020649,0.049645], \n",
    "        '3':[0.247788, 0.319149], '4':[0.466077,0.397163],\n",
    "        '5 Extremely confident': [0.244838, 0.205674]} \n",
    "\n",
    "df_confidence_percentage = pd.DataFrame(confidence_data , index = [\"Native\", \"Non-Native\"])\n",
    "df_confidence_percentage.plot.barh(stacked=True, colormap='RdYlGn').legend(bbox_to_anchor=(1, 0.5))\n",
    "plt.title('Confidence Interval ')\n",
    "plt.xlabel('Percentage of Confidence Interval')\n",
    "plt.ylabel('Type of English Speaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXN7cd_cxwdf"
   },
   "source": [
    "### Color Maps\n",
    "[Colors](https://matplotlib.org/tutorials/colors/colormaps.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2iZLSQQYCD75",
    "outputId": "ee1642ba-4e84-4ef9-b7a6-2e97d6badee9"
   },
   "outputs": [],
   "source": [
    "print(\"The Average Native English Speaker Confidence Level is: \" + str(np.mean(df_confidence_n.confidence_level)))\n",
    "print(\"the stdv of native English Speakers Confidenec Level is: \" + str(df_confidence_n.confidence_level.std()))\n",
    "      \n",
    "print(\"The Average Non-Native English Speaker Confidence Level is: \" + str(np.mean(df_confidence_nn.confidence_level)))\n",
    "print(\"the stdv of Non-Native English Speakers Confidenec Level is: \" + str(df_confidence_nn.confidence_level.std()))\n",
    "\n",
    "print(\"Running Tests to see if they are significantly different\")\n",
    "# run_welch_test(df_confidence_n.overall_grades, df_confidence_nn.overall_grades)\n",
    "# run_t_test(df_confidence_n.overall_grades, df_confidence_nn.overall_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "4jX-tZU5oj_u",
    "outputId": "d8b4be13-5225-41b2-820a-dd736d247a29"
   },
   "outputs": [],
   "source": [
    "# df['native_bool']  = (df['english_fluency'] == 4) \n",
    "# df.loc[(df['english_fluency'] == 3), ['native_bool']]= True \n",
    "# dfc = df.copy()\n",
    "# dfc_full = dfc.copy()     \n",
    "\n",
    "# # dfc_full = impute_from_distrib(dfc_full, 'confidence_level')\n",
    "# describe_col(dfc_full, 'confidence_level')\n",
    "\n",
    "# mean_confidence_native = get_single_mean(dfc_full[dfc_full['native_bool'] == True], 'confidence_level')\n",
    "# mean_confidence_non_native = get_single_mean(dfc_full[dfc_full['native_bool'] == False], 'confidence_level')\n",
    "\n",
    "# print(\"The mean confidence level for native english speakers is \" + str(mean_confidence_native))\n",
    "# print(\"The mean confidence level for non-native english speakers is \" + str(mean_confidence_non_native))\n",
    "\n",
    "# print(\"---------------------T-Test---------------------\")\n",
    "# run_t_test(dfc_full[dfc_full['native_bool'] == True].confidence_level, dfc_full[dfc_full['native_bool'] == False].confidence_level)\n",
    "# print(\"---------------------Welch Test-------------------\")\n",
    "# run_welch_test(dfc_full[dfc_full['native_bool'] == True].confidence_level, dfc_full[dfc_full['native_bool'] == False].confidence_level)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TixL6adYuBrL"
   },
   "source": [
    "### A: No, the non-native english speakers do not have a statistically significant lower overall confidence level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8IAbQ30co6F"
   },
   "source": [
    "### ANCOVA. Do non-native English speakers have a statistically significant lower overall confidence level after controlling for gender and prior experience?\n",
    "\n",
    "It was shown that prior experience and gender were strongly correlated with confidence level. We will use ANCOVA to determine whether the lower overall confidence level of non-native English speakers is still statistically significant by controlling for gender and prior experience. \n",
    "\n",
    "For this analysis: gender and prior experience are the covariates, language fluency is the independent variable, and confidence level is the dependent variable. \n",
    "\n",
    "Assumptions for ANCOVA:\n",
    " http://web.mnstate.edu/malonech/Psy633/Notes/Mertler%20&%20Vannatta%20Ch.%205%20ANCOVA.pdf\n",
    " \n",
    "http://oak.ucc.nau.edu/rh232/courses/eps625/handouts/ancova/understanding%20ancova.pdf\n",
    " 1.) normality of data (dependent variable - confidence level) for any level of the independent variable. Will be assessed by visual inspection and q-q plots.\n",
    "2.)Assumption of equal variances. To test if the overall confidence levels of each group is equal we will use the Levene Test. \n",
    "3.) Assumption that the relationship between the covariate and dependent variable is linear. To test this assumption we will use the ordinary least squares function.  \n",
    "4.) Assumption of Homogeneity of Regression slopes will be tested by checking that the means of the covariates is equal across both factors (indpendent variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n4-2uRctye0M",
    "outputId": "cee03011-8099-4a44-921a-fc07d863194a"
   },
   "outputs": [],
   "source": [
    "\n",
    "dfa = dfc_full.copy(deep = True)\n",
    "dfa = dfa.dropna(subset=['native_bool'])\n",
    "\n",
    "# ##check for empty values of native bool and confidence level\n",
    "# #describe_col(dfa,'native_bool')\n",
    "# #describe_col(dfa,'confidence_level')\n",
    "\n",
    "#-----------Check if result changes with imputation\n",
    "dfa = impute_from_distrib(dfa, 'confidence_level')\n",
    "#describe_col(dfa,'confidence_level')\n",
    "\n",
    "# ##-----------Check if result changes with dropping empty rows\n",
    "# # dfa = dfa.dropna(subset=['confidence_level'])\n",
    "# # describe_col(dfa,'confidence_level')\n",
    "\n",
    "df_d = df_d.rename(columns = {\"Gender (Binary)\":'gender'})\n",
    "dfa = (dfa.set_index('id').join(df_d.set_index('DeID') ,how='left'))\n",
    "\n",
    "#coding the native_bool column Native = 1, Non-Native = 0\n",
    "dfa['native_bool_num'] = ''\n",
    "dfa.loc[(dfa['native_bool'] == True ), ['native_bool_num']]= 1\n",
    "dfa.loc[(dfa['native_bool'] == False ), ['native_bool_num']]= 0\n",
    "dfa[['native_bool_num']] = dfa[['native_bool_num']].apply(pd.to_numeric)\n",
    "\n",
    "#coding the gender column male - 3, female - 2, non-binary - 1\n",
    "dfa=dfa.dropna(subset=['gender']) #drop empty\n",
    "dfa['gender_num'] = 0\n",
    "dfa.loc[(dfa['gender'] == 'M' ), ['gender_num']]= 3\n",
    "dfa.loc[(dfa['gender'] == 'F' ), ['gender_num']]= 2\n",
    "dfa.loc[(dfa['gender'] == 'Non-Binary' ), ['gender_num']]= 1\n",
    "dfa[['gender']] = dfa[['gender_num']].apply(pd.to_numeric)\n",
    "print(\"total Students is \" + str(dfa.shape[0]))\n",
    "\n",
    "# describe_col(dfa,'gender_num')\n",
    "##----------un-comment this section to check results using imputation-----------\n",
    "dfa = impute_from_distrib(dfa,'gender_num')\n",
    "dfa = impute_from_distrib(dfa,'prior_exp')\n",
    "\n",
    "# ##----------un-comment this section to check results by dropping rows with empty values\n",
    "# dfa=dfa.dropna(subset=['confidence_level'])\n",
    "\n",
    "##separate dataframes for independent variable \n",
    "dfa_nn = dfa[dfa['native_bool_num'] == 0 ]\n",
    "dfa_ne = dfa[dfa['native_bool_num'] == 1 ]\n",
    "print(\"total NN Students is \" + str(dfa_nn.shape[0]))\n",
    "print(\"total NE Students is \" + str(dfa_ne.shape[0]))\n",
    "\n",
    "# describe_col(dfa,'gender_num')\n",
    "# display(dfa)\n",
    "#print out the data types\n",
    "# print(dfa.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hx3hAqty2b23"
   },
   "source": [
    "#### Assumption 1.) Test for normality of the dependent variable (confidence_level) for any level of independent variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbFkzc3EDe15"
   },
   "source": [
    "a.) Visual inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "Daxy-RpG2UEM",
    "outputId": "6c3b1e78-cd70-4c7b-ca6f-10308d0404ad"
   },
   "outputs": [],
   "source": [
    "\n",
    "#visual inspection of confidence level normality for native-English speaker population\n",
    "dist_ne = sns.distplot(dfa_ne['confidence_level'], bins = 50,  color=\"skyblue\", label=\"Confidence Level of Native English Speakers\")\n",
    "dist_ne.set_title('Distribution of Confidence Levels For Native English Speakers')\n",
    "dist_ne.set_ylabel('number of students')\n",
    "dist_ne.set_xlabel('confidence levels')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "12wfrhk3Ht90",
    "outputId": "fff07944-a8d5-4a3f-ec43-8bb9d2592440"
   },
   "outputs": [],
   "source": [
    "#visual inspection of confidence level normality for non-native speaker population\n",
    "dist_nn = sns.distplot(dfa_nn['confidence_level'], bins = 50, color=\"red\", label=\"Confidence Level of Non-Native English Speakers\")\n",
    "dist_nn.set_title('Distribution of Confidence Levels For Non_Native English Speakers')\n",
    "dist_nn.set_ylabel('number of students')\n",
    "dist_nn.set_xlabel('confidence levels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKwmL59UDlW-"
   },
   "source": [
    "b.) QQ plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "8D86qhKDBzip",
    "outputId": "776981a5-8ec8-47eb-f22f-5a8edaadbdaa"
   },
   "outputs": [],
   "source": [
    "# QQ plots to check for normality of confidence level for any level of independent variable\n",
    "from scipy import stats\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "print('top Left: confidence level of non-native, top Right: confidence level of native')\n",
    "print('bottow Left: confidence level of all')\n",
    "\n",
    "nn_qq_plot = stats.probplot(dfa_nn['confidence_level'], plot=sns.mpl.pyplot)\n",
    "plt.subplot(2,2,2)\n",
    "ne_qq_plot = stats.probplot(dfa_ne['confidence_level'], plot=sns.mpl.pyplot)\n",
    "plt.subplot(2,2,3)\n",
    "all_qq_plot = stats.probplot(dfa['confidence_level'], plot=sns.mpl.pyplot)\n",
    "\n",
    "##---------------un-comment to produce a single quantile plot (same as above)\n",
    "#import numpy as np \n",
    "#import pylab \n",
    "#import scipy.stats as stats\n",
    "\n",
    "##-----------------change the dataframe to show for a different population \n",
    "# stats.probplot(dfa_nn['confidence_level'], dist=\"norm\", plot=pylab)\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fr-828PpP7F"
   },
   "source": [
    "#### Assumption 2.) Test for equal variance of the dependent variable (confidence interval) across groups using the Levene test.\n",
    "Levene test assumes normality. The null hypothesis is that the variances are equal. A significant result (p< 0.05) would indicate that we can reject the null hypothesis (variances are not equal).\n",
    "If we rejecting the null hypothesis then we would be in violation of the homogeneity of equal variances. \n",
    "If we cannot reject the null hypothesis then we can assume the variances are equal and proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_Levene_test(dfa_ne['confidence_level'],dfa_nn['confidence_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBhaXTnQ0sYz"
   },
   "source": [
    "#### Assumption 3.) Test that there is a linear relationship between the covariate (gender, prior experience) and the dependent variable (confidence level) for each level of the independent variable. **Can ignore if covariate is categorical variable.\n",
    "\n",
    "To check this assumption we conduct a correlation analysis between the dependent variable(s) and the covariate(s). They should be highly correlated.** \n",
    "\n",
    "a.) We confirm that the correlation coefficient suggests there is a correlation (which it does in this case) between the covariates and the dependent variable.\n",
    "\n",
    "b.) \"More precisely one should test this assumption by obtaining and examining residuals plots comparing the stndardized residuals to the predicted values for the dependent variable.\" - http://web.mnstate.edu/malonech/Psy633/Notes/Mertler%20&%20Vannatta%20Ch.%205%20ANCOVA.pdf\n",
    "This video explains how to analyze the residual plots: https://www.youtube.com/watch?v=iMdtTCX2Q70\n",
    "\n",
    "c.) We visually confirm that the scatter plot of the dependent variable(s) and the covariate(s) show there is a correlation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "sjXRvjJgAco3",
    "outputId": "58c84a92-9927-4a9e-ff34-23460de622b4"
   },
   "outputs": [],
   "source": [
    "covariate = 'prior_exp'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "x_nn=dfa_nn[covariate]\n",
    "y_nn=dfa_nn[dependent_var]\n",
    "\n",
    "x_ne=dfa_ne[covariate]\n",
    "y_ne=dfa_ne[dependent_var]\n",
    "\n",
    "#create a scatter plot \n",
    "plt.plot(x_nn, y_nn, 'o')\n",
    "plt.plot(x_ne, y_ne, '*')\n",
    "\n",
    "#plot a first degree polynomial m = slope, b=intercept\n",
    "m_nn, b_nn = np.polyfit(x_nn,y_nn, 1,)\n",
    "plt.plot(x_nn, m_nn*x_nn + b_nn, label = \"NNES\", c = 'lightblue')\n",
    "\n",
    "m_ne, b_ne = np.polyfit(x_ne,y_ne, 1,)\n",
    "plt.plot(x_ne, m_ne*x_ne + b_ne, label = \"NES\", c = 'coral')\n",
    "plt.xlabel(covariate)\n",
    "plt.ylabel(dependent_var)\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Regression Slopes: Confidence Level vs Prior Experience of NNES and NES\")\n",
    "plt.show()\n",
    "\n",
    "#return the slope of the line\n",
    "print(\"The slope of \" + dependent_var + \" vs \" + covariate + \" for NES is \"+ str(m_ne))\n",
    "print(\"The slope of \" + dependent_var + \" vs \" + covariate + \" for NNES is \"+ str(m_nn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "69dOzXf-AzDN",
    "outputId": "07674d19-44cc-4a30-8227-3c1fef76dda7"
   },
   "outputs": [],
   "source": [
    "#visual inspection for linearity\n",
    "covariate = 'prior_exp'\n",
    "dependent_var = 'confidence_level'\n",
    "sns.regplot(x=dfa_ne[covariate], y=dfa_ne[dependent_var],scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"}).set_title('Confidence Level vs Prior Experience of NES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "liebOA6WAkOb",
    "outputId": "5660fc1d-2726-4d68-dca7-e653c568f4e0"
   },
   "outputs": [],
   "source": [
    "##visual inspection for linearity\n",
    "covariate = 'prior_exp'\n",
    "dependent_var = 'confidence_level'\n",
    "sns.regplot(x=dfa_nn[covariate], y=dfa_nn[dependent_var],scatter_kws={\"color\": \"green\"}, line_kws={\"color\": \"coral\"}).set_title('Confidence Level vs Prior Experience of NNES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "colab_type": "code",
    "id": "otg9capmAMkW",
    "outputId": "04ee5ef9-8de8-43fd-ad50-3287fb65db8d"
   },
   "outputs": [],
   "source": [
    " # residuals plot NES prior experience\n",
    "covariate = 'prior_exp'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "X = dfa_ne['prior_exp']\n",
    "Y = dfa_ne['confidence_level']\n",
    "\n",
    "sns.residplot(X,Y, order = 1,color=\"g\").set_title('Residuals Plot: CL vs PE for NES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "dDiD0fvP6rSR",
    "outputId": "f2b1301d-3605-4318-9682-3de7c768ebbe"
   },
   "outputs": [],
   "source": [
    "# residuals plot NNES prior experience\n",
    "covariate = 'prior_exp'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "X = dfa_nn['prior_exp']\n",
    "Y = dfa_nn['confidence_level']\n",
    "\n",
    "sns.residplot(X,Y, order = 1,color=\"black\").set_title('Residuals Plot: CL vs PE for NNES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "GEPYbpT26_93",
    "outputId": "9e7cd3c0-73d8-46de-9f9d-ea474e4d389f"
   },
   "outputs": [],
   "source": [
    "\n",
    "covariate = 'gender_num'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "x_ne=dfa_ne[covariate]\n",
    "y_ne=dfa_ne[dependent_var]\n",
    "\n",
    "x_nn = dfa_nn[covariate]\n",
    "y_nn = dfa_nn[dependent_var]\n",
    "\n",
    "#create a scatter plot \n",
    "plt.plot(x_ne, y_ne, 'o')\n",
    "plt.plot(x_nn, y_nn, '*')\n",
    "\n",
    "#plot a first degree polynomial m = slope, b=intercept\n",
    "m_ne, b_ne = np.polyfit(x_ne,y_ne, 1)\n",
    "m_nn, b_nn = np.polyfit(x_nn,y_nn, 1)\n",
    "plt.plot(x_ne, m_ne*x_ne + b_ne, label = 'NES',c = 'blue')\n",
    "plt.plot(x_nn, m_nn*x_nn + b_nn, label = 'NNES',c = 'red')\n",
    "plt.xlabel(\"gender\")\n",
    "plt.ylabel(\"confidence level\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('Regression Slopes: Confidence Level vs Gender for NES and NNES')\n",
    "plt.show()\n",
    "\n",
    "#return the slope of the line\n",
    "print(\"The slope of \" + dependent_var + \" vs \" + covariate + \" for NES is \"+ str(m_ne))\n",
    "print(\"The slope of \" + dependent_var + \" vs \" + covariate + \" for NNES is \"+ str(m_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5atvDSA_0zA"
   },
   "outputs": [],
   "source": [
    "#visual inspection for linearity gender non natives\n",
    "covariate = 'gender_num'\n",
    "dependent_var = 'confidence_level'\n",
    "sns.regplot(x=dfa_nn[covariate], y=dfa_nn[dependent_var],scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"}).set_title('Confidence Level vs Gender for NNES');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oTlEsSZ7gRO"
   },
   "outputs": [],
   "source": [
    "#visual inspection for linearity gender NES\n",
    "covariate = 'gender_num'\n",
    "dependent_var = 'confidence_level'\n",
    "sns.regplot(x=dfa_ne[covariate], y=dfa_ne[dependent_var],scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"}).set_title('Confidence Level vs Gender for NES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "moUBZXUSI0NY",
    "outputId": "89ce1ab9-dab5-4c11-824a-42de8ebcc3a0"
   },
   "outputs": [],
   "source": [
    "# residuals plot NNES gender\n",
    "covariate = 'gender_num'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "X = dfa_nn[covariate]\n",
    "Y = dfa_nn[dependent_var]\n",
    "\n",
    "sns.residplot(X,Y, order = 1,color=\"black\").set_title('Residuals Plot: Confidence Level vs Gender for NNES');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W69-1COR1ZfA"
   },
   "outputs": [],
   "source": [
    "# residuals plot NES gender\n",
    "covariate = 'gender_num'\n",
    "dependent_var = 'confidence_level'\n",
    "\n",
    "X = dfa_ne[covariate]\n",
    "Y = dfa_ne[dependent_var]\n",
    "\n",
    "sns.residplot(X,Y, order = 1,color=\"black\").set_title('Residuals Plot: Confidence Level vs Gender for NES');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSRn9aLsUsRA"
   },
   "source": [
    "#### Assumption 5) Test for the homogeneity of regression slopes. \n",
    "Observe the regression slope plots above, if the lines do not cross then assumption is fulfilled.\n",
    "\n",
    "This assumption states that the regression slopes (best fit lines between the covariate and the dependent variable are equal for each group of the analysis. If an independent variable-covariate interaction exists, the relationship between the covariate and the dependent variable is different at different levels of the independent variable. \n",
    "For example, we must check that the regression slope of the gender-confidence level is the same for both the non-native and native populations. \n",
    "http://web.mnstate.edu/malonech/Psy633/Notes/Mertler%20&%20Vannatta%20Ch.%205%20ANCOVA.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "Gyp1MTHiilcb",
    "outputId": "1e81beab-840c-491c-ad28-f97e5d32ced0"
   },
   "outputs": [],
   "source": [
    "\n",
    "#covariates should have equal mean scores across the factor groups.\n",
    "#covariates are gender and prior experience\n",
    "cov_1 = 'gender_num'\n",
    "cov_2 = 'prior_exp'\n",
    "\n",
    "#getting the means of each covariate for each factor level\n",
    "dfa_ne_cov_1 = dfa_ne[cov_1].mean()\n",
    "dfa_nn_cov_1 = dfa_nn[cov_1].mean()\n",
    "dfa_ne_cov_2 = dfa_ne[cov_2].mean()\n",
    "dfa_nn_cov_2 = dfa_nn[cov_2].mean()\n",
    "\n",
    "print(\"covariate: \" + cov_1 + \" mean for native speakers is \"+ str(dfa_ne_cov_1))\n",
    "print(\"covariate: \" + cov_1 + \" mean for non-native speakers is \" + str(dfa_nn_cov_1))\n",
    "\n",
    "t_test_cov_1 = ttest_ind(dfa_ne[cov_1],dfa_nn[cov_1], equal_var=False)\n",
    "\n",
    "if (t_test_cov_1.pvalue < 0.05):\n",
    "  print('Reject the null hypothesis that the means of '+ cov_1 + ' are equal for native and non-native.')\n",
    "  print(str(t_test_cov_1.pvalue) + \" < 0.05\")\n",
    "  print('T-test results for average ' + cov_1 + ' between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "else:\n",
    "  print(\"We cannot reject the null hypotheis that the means for \" + cov_1 + ' are equal for native and non-native')\n",
    "  print(str(t_test_cov_1.pvalue) + ' > 0.05')\n",
    "  print('T-test results for average ' + cov_1 + ' between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "\n",
    "print('-----------------------------------------------------------------------')\n",
    "# now checking prior experience\n",
    "\n",
    "print(\"covariate: \" + cov_2 + \" mean for native speakers is \"+ str(dfa_ne_cov_2))\n",
    "print(\"covariate: \" + cov_2 + \" mean for non-native speakers is \"+ str(dfa_nn_cov_2))\n",
    "\n",
    "t_test_cov_2 = ttest_ind(dfa_ne[cov_2],dfa_nn[cov_2], equal_var=False)\n",
    "\n",
    "if (t_test_cov_2.pvalue < 0.05):\n",
    "  print(\"Reject the null hypothesis that the means of \" + cov_2 + \" are equal for native and non-native.\")\n",
    "  print(str(t_test_cov_2.pvalue) + \"< 0.05\")\n",
    "  print('T-test results for average ' + cov_2 + ' between native and non-native:')\n",
    "  print(str(t_test_cov_2))\n",
    "else:\n",
    "  print(\"We cannot reject the null hypotheis that the means for \" + cov_2 + \" are equal for native and non-native\")\n",
    "  print(str(t_test_cov_2.pvalue) + '> 0.05')\n",
    "  print('T-test results for average ' + cov_2 + ' between native and non-native:')\n",
    "  print(str(t_test_cov_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "NCTe7iAoPoR2",
    "outputId": "336b32d5-a811-4b8b-fb29-b8bce5497771"
   },
   "outputs": [],
   "source": [
    "!pip install pingouin \n",
    "\n",
    "# x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiUpStkTGa_M"
   },
   "source": [
    "#### ANCOVA \n",
    "Rows with missing values are automatically removed (listwise deletion) by the ancova function as stated in the documentation. \n",
    "\n",
    "info on eta squared: \n",
    "the percentage of variance in the dependent variable accounted for by the independent variable (similar to r^2)\n",
    "\n",
    "partial eta squared is the same as eta squared when you only have one independent variable\n",
    "https://www.statisticshowto.com/eta-squared/\n",
    "\n",
    "to analyze the ancova table:\n",
    "https://www.sheffield.ac.uk/polopoly_fs/1.531229!/file/MASH_ANCOVA_SPSS.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "EYFYMn9EhOot",
    "outputId": "e9d1c0cb-8933-4edb-a93d-379c951d0c48"
   },
   "outputs": [],
   "source": [
    "\n",
    "#checking the interaction of the independent variable with the covariates\n",
    "import pingouin as pg\n",
    "from pingouin import ancova\n",
    "\n",
    "c1 = 'gender_num'\n",
    "c2 = 'prior_exp'\n",
    "d_var = 'confidence_level'\n",
    "\n",
    "# we select partial eta squared because \n",
    "ancova_result = ancova(data=dfa, dv= d_var, covar=[ c1, c2], between='native_bool_num', effsize=\"np2\")\n",
    "\n",
    "display(ancova_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANCOVA Results indicate that we can reject the hypothesis that english language fluency leads to the same confidence level after controlling for gender and prior experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LO6_iTkGlip"
   },
   "source": [
    "##### Multiple Regression Results\n",
    "\n",
    "The ANCOVA assumption were fulfilled and ANCOVA was performed, but we proceed with exploring Multiple Regression. When using multiple regression it is ideal that the variables gender num, native bool num, and prior experience not be correlated with each other. This has not been checked.\n",
    "**non-imputed data:** \n",
    "**gender num coeff 0.24 pval .00429** ,\n",
    "**native bool num 0.03 pval 0.7,**\n",
    "**prior exp 0.42 pval 5.35 e-28** \n",
    "\n",
    "0.42 is an estimate of the expected increase in confidence level corresponding to a unit increase in prior experience when gender_num and native_bool_num are held constant. For example, if a student's prior experience was increased by 1, then their confidence level would increase by 0.42, while other variables are constant. **Prior experience has the highest impact on confidence level, followed by gender, and lastly English fluency (not significant)**. \n",
    "\n",
    "**imputed data:**\n",
    "gender num coeff 0.23,\n",
    "native bool num 0.04,\n",
    "prior exp 0.42.\n",
    "\n",
    "When using imputed data the native bool influence increases and gender decreases slightly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5vYBnXIWMPf"
   },
   "outputs": [],
   "source": [
    "#run this section to check multiple regression with imputation\n",
    "dfm = dfa.copy(deep=True)\n",
    "\n",
    "#coding the native_bool column Native = 1, Non-Native = 0\n",
    "dfm['native_bool_num'] = ''\n",
    "dfm.loc[(dfm['native_bool'] == True ), ['native_bool_num']]= 1\n",
    "dfm.loc[(dfm['native_bool'] == False ), ['native_bool_num']]= 0\n",
    "dfm[['native_bool_num']] = dfm[['native_bool_num']].apply(pd.to_numeric)\n",
    "\n",
    "#coding the gender column male - 3, female - 2, non-binary - 1\n",
    "dfm=dfm.dropna(subset=['gender']) #drop empty\n",
    "dfm['gender_num'] = 0\n",
    "dfm.loc[(dfm['gender'] == 'Male' ), ['gender_num']]= 3\n",
    "dfm.loc[(dfm['gender'] == 'Female' ), ['gender_num']]= 2\n",
    "dfm.loc[(dfm['gender'] == 'Non-Binary' ), ['gender_num']]= 1\n",
    "dfm[['gender_num']] = dfm[['gender_num']].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "##----------imputation-----------\n",
    "dfm = impute_from_distrib(dfm,'gender_num')\n",
    "dfm = impute_from_distrib(dfm,'prior_exp')\n",
    "# dfm = impute_from_distrib(dfm,'confidence_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "suIIdxm4qQqw",
    "outputId": "cd6664fd-d288-4ed4-8f4e-7152891af15a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# #---- uncomment this section to check multiple regression using IMPUTED data ------\n",
    "#----------imputation-----------\n",
    "dfm = impute_from_distrib(dfm,'gender_num')\n",
    "dfm = impute_from_distrib(dfm,'prior_exp')\n",
    "dfm = impute_from_distrib(dfm,'confidence_level')\n",
    "lm = pg.linear_regression(dfa[['gender_num', 'native_bool_num', 'prior_exp']], dfa['confidence_level'], )\n",
    "lm \n",
    "\n",
    "# #----- uncomment this section to check multiple regression using NON IMPUTED data --------\n",
    "# dfm=dfm.dropna(subset=['prior_exp'])\n",
    "# lm = pg.linear_regression(dfm[['gender_num', 'native_bool_num', 'prior_exp']], dfm['confidence_level'], )\n",
    "# lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gYjEiybSOhSR",
    "outputId": "6a4c76db-5d2a-4442-c65b-184e2b57ba52"
   },
   "outputs": [],
   "source": [
    "#This code provides the important coefficients for our variables.\n",
    "dfa=dfa.dropna(subset=['prior_exp'])\n",
    "X= dfa[['gender_num', 'native_bool','prior_exp']]\n",
    "Y = dfa['confidence_level']\n",
    "\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: Is the prior experience of Native English Speakers different from non-native English Speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = dfc_full.copy(deep=True)\n",
    "\n",
    "# describe_col(dfp,'native_bool')\n",
    "# dfp = dfp.dropna(subset=['native_bool'])\n",
    "\n",
    "df_d = df_d.rename(columns = {\"Gender (Binary)\":'gender'})\n",
    "dfp = (dfp.set_index('id').join(df_d.set_index('DeID') ,how='left'))\n",
    "\n",
    "# ##-----------Check for empty values of gender and prior experience\n",
    "# describe_col(dfp,'gender')\n",
    "# describe_col(dfp,'prior_exp')\n",
    "\n",
    "# ##-----------Check if result changes with imputation of gender\n",
    "# #Since there are 37 rows empty for gender and 2 empty for prior experience we will impute all\n",
    "dfp = impute_from_distrib(dfp, 'gender')\n",
    "# describe_col(dfp,'gender')\n",
    "\n",
    "# #-----------Check if result changes with dropping empty rows of gender\n",
    "# dfp=dfp.dropna(subset=['gender']) #drop empty\n",
    "# describe_col(dfp,'gender')\n",
    "\n",
    "#-----------Check if result changes with imputation of prior exp\n",
    "dfp = impute_from_distrib(dfp, 'prior_exp')\n",
    "#describe_col(dfp,'prior_exp')\n",
    "\n",
    "# #-----------Check if result changes with dropping empty rows of prior exp\n",
    "# dfp=dfp.dropna(subset=['prior_exp']) #drop empty\n",
    "\n",
    "\n",
    "#coding the native_bool column Native = 1, Non-Native = 0\n",
    "dfp['native_bool_num'] = ''\n",
    "dfp.loc[(dfp['native_bool'] == True ), ['native_bool_num']]= 1\n",
    "dfp.loc[(dfp['native_bool'] == False ), ['native_bool_num']]= 0\n",
    "dfp[['native_bool_num']] = dfp[['native_bool_num']].apply(pd.to_numeric)\n",
    "\n",
    "#coding the gender column male - 3, female - 2, non-binary - 1\n",
    "dfp['gender_num'] = 0\n",
    "dfp.loc[(dfp['gender'] == 'M' ), ['gender_num']]= 3\n",
    "dfp.loc[(dfp['gender'] == 'F' ), ['gender_num']]= 2\n",
    "dfp.loc[(dfp['gender'] == 'Non-Binary' ), ['gender_num']]= 1\n",
    "dfp[['gender']] = dfp[['gender_num']].apply(pd.to_numeric)\n",
    "print(\"total Students is \" + str(dfp.shape[0]))\n",
    "\n",
    "\n",
    "##separate dataframes for independent variable \n",
    "dfp_nn = dfp[dfp['native_bool_num'] == 0 ]\n",
    "dfp_ne = dfp[dfp['native_bool_num'] == 1 ]\n",
    "print(\"total NN Students is \" + str(dfp_nn.shape[0]))\n",
    "print(\"total NE Students is \" + str(dfp_ne.shape[0]))\n",
    "\n",
    "# describe_col(dfa,'gender_num')\n",
    "# display(dfa)\n",
    "#print out the data types\n",
    "# print(dfa.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_ne_mean_p = dfp_ne['prior_exp'].mean()\n",
    "dfp_nn_mean_p = dfp_nn['prior_exp'].mean()\n",
    "\n",
    "\n",
    "print(\"prior experience mean for native speakers is \"+ str(dfp_ne_mean_p))\n",
    "print(\"prior experience mean for non-native speakers is \" + str(dfp_nn_mean_p))\n",
    "\n",
    "t_test_cov_1 = ttest_ind(dfp_ne['prior_exp'],dfp_nn['prior_exp'], equal_var=False)\n",
    "\n",
    "if (t_test_cov_1.pvalue < 0.05):\n",
    "  print('Reject the null hypothesis that the means of prior experience are equal for native and non-native.')\n",
    "  print(str(t_test_cov_1.pvalue) + \" < 0.05\")\n",
    "  print('T-test results for average prior experience between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "else:\n",
    "  print('Decision: Fail to Reject that the means for prior experience are equal for native and non-native')\n",
    "  print(str(t_test_cov_1.pvalue) + ' > 0.05')\n",
    "  print('T-test results for average prior experience between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "    \n",
    "print('----------------Mann-Whitney U test---------------')\n",
    "#input non-natives then natives\n",
    "run_Mann_Whitney_test(dfp_nn['prior_exp'],dfp_ne['prior_exp'])\n",
    "\n",
    "print('-----------------------------------------------------------------------')\n",
    "# now checking prior experience\n",
    "dfp_ne_mean_gender = dfp_ne['gender_num'].mean()\n",
    "dfp_nn_mean_gender = dfp_nn['gender_num'].mean()\n",
    "\n",
    "\n",
    "print(\"gender mean for native speakers is \"+ str(dfp_ne_mean_gender))\n",
    "print(\"gender mean for non-native speakers is \" + str(dfp_nn_mean_gender))\n",
    "\n",
    "t_test_cov_2 = ttest_ind(dfp_ne['gender_num'],dfp_nn['gender_num'], equal_var=False)\n",
    "\n",
    "if (t_test_cov_2.pvalue < 0.05):\n",
    "  print(\"Reject the null hypothesis that the means of gender are equal for native and non-native.\")\n",
    "  print(str(t_test_cov_2.pvalue) + \"< 0.05\")\n",
    "  print('T-test results for average gender between native and non-native:')\n",
    "  print(str(t_test_cov_2))\n",
    "else:\n",
    "  print(\"Decision: Fail to Reject that the means for gender are equal for native and non-native\")\n",
    "  print(str(t_test_cov_2.pvalue) + '> 0.05')\n",
    "  print('T-test results for average gender between native and non-native:')\n",
    "  print(str(t_test_cov_2))\n",
    "\n",
    "print('----------------Mann-Whitney U test---------------')\n",
    "#input non-natives then natives\n",
    "run_Mann_Whitney_test(dfp_nn['gender'],dfp_ne['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fail to reject that the mean of gender and prior expeience is equal for native and non native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs = dfc_full.copy(deep=True)\n",
    "\n",
    "# describe_col(dfp,'native_bool')\n",
    "# dfp = dfp.dropna(subset=['native_bool'])\n",
    "\n",
    "df_d = df_d.rename(columns = {\"Gender (Binary)\":'gender'})\n",
    "df_cs = (df_cs.set_index('id').join(df_d.set_index('DeID') ,how='left'))\n",
    "\n",
    "# ##-----------Check for empty values of cs experience\n",
    "# describe_col(df_cs,'cs_exp')\n",
    "\n",
    "#There are 9 empty rows of cs experience\n",
    "\n",
    "# #-----------Check if result changes with dropping empty rows of cs exp\n",
    "# df_cs=df_cs.dropna(subset=['cs_exp']) #drop empty\n",
    "# describe_col(df_cs,'cs_exp')\n",
    "\n",
    "##-----------Check if result changes with imputation of prior exp\n",
    "df_cs = impute_from_distrib(df_cs, 'cs_exp')\n",
    "describe_col(df_cs,'cs_exp')\n",
    "\n",
    "#coding the native_bool column Native = 1, Non-Native = 0\n",
    "df_cs['native_bool_num'] = ''\n",
    "df_cs.loc[(df_cs['native_bool'] == True ), ['native_bool_num']]= 1\n",
    "df_cs.loc[(df_cs['native_bool'] == False ), ['native_bool_num']]= 0\n",
    "df_cs[['native_bool_num']] = df_cs[['native_bool_num']].apply(pd.to_numeric)\n",
    "\n",
    "#coding the gender column male - 3, female - 2, non-binary - 1\n",
    "df_cs['gender_num'] = 0\n",
    "df_cs.loc[(df_cs['gender'] == 'M' ), ['gender_num']]= 3\n",
    "df_cs.loc[(df_cs['gender'] == 'F' ), ['gender_num']]= 2\n",
    "df_cs.loc[(df_cs['gender'] == 'Non-Binary' ), ['gender_num']]= 1\n",
    "df_cs[['gender']] = df_cs[['gender_num']].apply(pd.to_numeric)\n",
    "print(\"total Students is \" + str(df_cs.shape[0]))\n",
    "\n",
    "\n",
    "##separate dataframes for independent variable \n",
    "dfp_nn = df_cs[df_cs['native_bool_num'] == 0 ]\n",
    "dfp_ne = df_cs[df_cs['native_bool_num'] == 1 ]\n",
    "print(\"total NN Students is \" + str(dfp_nn.shape[0]))\n",
    "print(\"total NE Students is \" + str(dfp_ne.shape[0]))\n",
    "\n",
    "# describe_col(dfa,'gender_num')\n",
    "# display(dfa)\n",
    "#print out the data types\n",
    "# print(dfa.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input is a fully imputed dataframe\n",
    "def get_cs_exp_frequency(df_input,col):\n",
    "    \"\"\"Gets the a frequency value for the cs experience\"\"\"\n",
    "    #initializing cs experince num col and making copy of input col\n",
    "    df_input['cs_exp_num'] = 0\n",
    "\n",
    "    for i in df_input[col]:\n",
    "        df_input['cs_exp_num'] = df_input[col].apply(lambda x: x.count('AP CS A'))\n",
    "        df_input['cs_exp_num'] += df_input[col].apply(lambda x: x.count('CS courses at a college or university AS A STUDENT AT THAT COLLEGE OR UNIVERSITY'))\n",
    "        df_input['cs_exp_num'] += df_input[col].apply(lambda x: x.count('Took one of more CS courses at a college or university AS A HIGH SCHOOL STUDENT '))\n",
    "        #df_input['cs_exp_num'] += df_input[col].apply(lambda x: x.count('AP CS Principles'))\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs_exp = get_cs_exp_frequency(df_cs,'cs_exp')\n",
    "\n",
    "df_cs_exp_ne = df_cs_exp[df_cs_exp['native_bool']==True]\n",
    "df_cs_exp_nn = df_cs_exp[df_cs_exp['native_bool']==False]\n",
    "#display(df_cs_exp_ne)\n",
    "#describe_col(df_cs_exp_ne,'cs_exp_num')\n",
    "\n",
    "print(\"cs experience mean for native speakers is \" +  str(df_cs_exp_ne['cs_exp_num'].mean()))\n",
    "print(\"cs experience mean for non-native speakers is \" + str(df_cs_exp_nn['cs_exp_num'].mean()))\n",
    "\n",
    "t_test_cov_1 = ttest_ind( df_cs_exp_ne['cs_exp_num'], df_cs_exp_nn['cs_exp_num'], equal_var=False)\n",
    "\n",
    "if (t_test_cov_1.pvalue < 0.05):\n",
    "  print('Reject the null hypothesis that the means of cs experience are equal for native and non-native.')\n",
    "  print(str(t_test_cov_1.pvalue) + \" < 0.05\")\n",
    "  print('T-test results for average cs experience between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "else:\n",
    "  print('Decision: Fail to Reject that the means for cs experience are equal for native and non-native')\n",
    "  print(str(t_test_cov_1.pvalue) + ' > 0.05')\n",
    "  print('T-test results for average cs experience between native and non-native:')\n",
    "  print(str(t_test_cov_1))\n",
    "    \n",
    "print('----------------Mann-Whitney U test---------------')\n",
    "#input non-natives then natives\n",
    "run_Mann_Whitney_test(df_cs_exp_nn['cs_exp_num'], df_cs_exp_ne['cs_exp_num'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_og.copy(deep = True)\n",
    "\n",
    "df_new=df_new.dropna(subset=['transfer']) #drop empty\n",
    "\n",
    "df_new['transfer_bool']= False\n",
    "\n",
    "df_new.loc[(df_new['transfer']=='Yes'),['transfer_bool']] = True\n",
    "\n",
    "#display(df_new)\n",
    "#df_new[df_new['transfer']== 'Yes']\n",
    "# #df_new.loc[(df_new['english_fluency'] == 3), ['native_bool']]= True \n",
    "# display(df_new)\n",
    "# #coding the native_bool column Native = 1, Non-Native = 0\n",
    "\n",
    "df_new['transfer_bool_num'] = ''\n",
    "df_new.loc[(df_new['transfer_bool'] == True ), ['transfer_bool_num']]= 1\n",
    "df_new.loc[(df_new['transfer_bool'] == False ), ['transfer_bool_num']]= 0\n",
    "df_new[['transfer_bool_num']] = df_new[['transfer_bool_num']].apply(pd.to_numeric)\n",
    "\n",
    "df_ne = df_new[df_new['native_bool']==True]\n",
    "df_nn = df_new[df_new['native_bool']==False]\n",
    "\n",
    "describe_col(df_nn,'transfer_bool_num')\n",
    "\n",
    "t_test= ttest_ind(df_nn['transfer_bool_num'],df_ne['transfer_bool_num'], equal_var=False)\n",
    "print(t_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNjNcGAY2FwP"
   },
   "source": [
    "## Q: Do non-native english students have a different minimum grade needed to be satisfied than native english speakers? \n",
    "\n",
    "Using the [grading system to quantify grades](https://blink.ucsd.edu/instructors/academic-info/grades/system.html#:~:text=Find%20out%20about%20the%20UCSD,and%20how%20to%20interpret%20grades.&text=The%20grade%20of%20A%2B%2C%20when,and%20minus%20suffixes%20is%20optional.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IY7EBY4aBlLX",
    "outputId": "1627d8cf-a449-4e87-d30a-ac139ce073e1"
   },
   "outputs": [],
   "source": [
    "dfge = df.copy()\n",
    "dfge['ordinal_min_grade'] = ''\n",
    "def quantify_expected_grade(input_dataframe): \n",
    "    \"\"\"Changed expected grade to numbers A+ = 4.0, etc \"\"\"\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'A+'), ['ordinal_min_grade']]= 4.3\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'A'), ['ordinal_min_grade']]= 4.0\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'A-'), ['ordinal_min_grade']]= 3.7\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'B+'), ['ordinal_min_grade']]= 3.3\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'B'), ['ordinal_min_grade']]= 3.0\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'B-'), ['ordinal_min_grade']]= 2.7\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'C+'), ['ordinal_min_grade']]= 2.3\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'C'), ['ordinal_min_grade']]= 2.0\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'C-'), ['ordinal_min_grade']]= 1.7\n",
    "    input_dataframe.loc[(input_dataframe['min_grade_expected'] ==  'D'), ['ordinal_min_grade']]= 1\n",
    "    input_dataframe[[\"ordinal_min_grade\"]] = input_dataframe[[\"ordinal_min_grade\"]].apply(pd.to_numeric)\n",
    "    return input_dataframe \n",
    "\n",
    "quantify_expected_grade(dfge)\n",
    "\n",
    "dfge = impute_from_distrib(dfge, 'ordinal_min_grade')\n",
    "describe_col(dfge, 'ordinal_min_grade')\n",
    "\n",
    "mean_expected_grade = get_single_mean(dfge[dfge['native_bool'] == True], 'ordinal_min_grade')\n",
    "mean_expected_grade_non_native = get_single_mean(dfge[dfge['native_bool'] == False], 'ordinal_min_grade')\n",
    "\n",
    "print(\"The mean min grade to satisfy native english speakers is \" + str(mean_expected_grade))\n",
    "print(\"The mean min grade to satisfy non-native english speakers is \" + str(mean_expected_grade_non_native))\n",
    "\n",
    "print(\"---------------------T-Test---------------------\")\n",
    "run_t_test(dfge[dfge['native_bool'] == True].ordinal_min_grade, dfge[dfge['native_bool'] == False].ordinal_min_grade)\n",
    "print(\"---------------------Welch Test-------------------\")\n",
    "run_welch_test(dfge[dfge['native_bool'] == True].ordinal_min_grade, dfge[dfge['native_bool'] == False].ordinal_min_grade)\n",
    "\n",
    "\n",
    "dfge_heatmap = dfge.copy()\n",
    "dfge_heatmap['gender_ordinal']=dfge_heatmap['gender'].astype('category').cat.codes\n",
    "# dfge_heatmap['transfer']=dfge_heatmap['transfer'].astype('category').cat.codes\n",
    "# dfge_heatmap['race']=dfge_heatmap['race'].astype('category').cat.codes\n",
    "plt.rc('figure', figsize=( 5,2)  , dpi=200)\n",
    "plt.rc('font', size=5)   # font size\n",
    "plt.xlabel(\"Values on X axis\")\n",
    "sns.heatmap(dfge_heatmap.corr(), annot = True, cmap= 'coolwarm', square=True, linewidths=.5,vmin=-1, vmax=1)\n",
    "# dfge_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "id": "gbkDzbyOF1S1",
    "outputId": "f480fb7f-e9da-493f-f46a-b92ecdca8408"
   },
   "outputs": [],
   "source": [
    "\n",
    "dfge = df.copy()\n",
    "grade_e_counts = (dfge[dfge['native_bool'] == True ].min_grade_expected.value_counts(normalize = True).sort_index())\n",
    "grade_e_counts_nn = (dfge[dfge['native_bool'] == False ].min_grade_expected.value_counts(normalize = True).sort_index())\n",
    "print(\"NATIVE\")\n",
    "print(grade_e_counts)\n",
    "print(\"NON-NATIVE\")\n",
    "print(grade_e_counts_nn)\n",
    "list_grade_e_counts_n = [0.007407, 0.266667, 0.288889, 0.103704, 0.192593, 0.074074, \n",
    "      0.014815, 0.022222, 0.029630, 0 ]\n",
    "list_grade_e_counts_nn = [0.026549, 0.345133, 0.398230, 0.061947, 0.115044, 0.035398, 0,\n",
    "      0.008850,0,  0.008850, ] \n",
    "graph_stack_grade_e = pd.DataFrame(index= ['Native', 'Non-Native'], columns= ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-' , 'D'])\n",
    "graph_stack_grade_e.loc['Native'] = (list_grade_e_counts_n ) # adding a row\n",
    "graph_stack_grade_e.loc['Non-Native'] = (list_grade_e_counts_nn ) # adding a row\n",
    "plt.style.use('seaborn-white')   # seaborn custom plot style\n",
    "plt.rc('figure', dpi=100, figsize=(7, 5))   # set default size/resolution\n",
    "plt.rc('font', size=12)   # font size\n",
    "graph_stack_grade_e.plot.barh(stacked=True, colormap='tab20c').legend(bbox_to_anchor=(1, 0.5))\n",
    "plt.title('CSE12 Min Grade Expected')\n",
    "plt.xlabel('Percentage of students')\n",
    "plt.ylabel('Type of English Speaker')\n",
    "graph_stack_grade_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYRVfLscgMMz"
   },
   "source": [
    "## Q: What is the strongest correlation coefficient that is associated with confidence level?  \n",
    "\n",
    ".cat.codes converts category from a string representation into an integer representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AUay5Wg2izif",
    "outputId": "2af73e63-e1b0-4b6a-8504-f86161e0138f"
   },
   "outputs": [],
   "source": [
    "dfg_cat = dfg.copy()\n",
    "dfg_cat['gender_ordinal']=dfg_cat['gender'].astype('category').cat.codes\n",
    "dfg_cat['transfer']=dfg_cat['transfer'].astype('category').cat.codes\n",
    "dfg_cat['race']=dfg_cat['race'].astype('category').cat.codes\n",
    "dfg_cat['ethnicity']=dfg_cat['ethnicity'].astype('category').cat.codes\n",
    "plt.rc('figure', figsize=( 6,3)  , dpi=200)\n",
    "plt.rc('font', size=5)   # font size\n",
    "plt.xlabel(\"Values on X axis\")\n",
    "sns.heatmap(dfg_cat.corr(), annot = True, cmap= 'coolwarm', square=True, linewidths=.5,vmin=-1, vmax=1)\n",
    "\n",
    "df_corr = dfg_cat.corr(method ='pearson')\n",
    "corr_confidence_level = df_corr['confidence_level'].sort_values(ascending=False)\n",
    "corr_grade = df_corr['overall_grades'].sort_values(ascending=False)\n",
    "print(corr_confidence_level)\n",
    "print(corr_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjaW3DUrFkB4"
   },
   "source": [
    "## A: The strongest correlation coefficient is prior experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSuMJCQl_2pj"
   },
   "source": [
    "# <a id='section_RQC'> Research Question Confidence Level + Grades </a>   \n",
    "\n",
    "[- Return](#toc) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODTnMXZAJD6J"
   },
   "source": [
    "### Does confidence level correlate with grade earned?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UptzRUpJIi6"
   },
   "source": [
    "### Is there a significant difference between the average confidence level of non-native and native english speaker? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "id": "dtQCQNzB_2pm",
    "outputId": "c53acc6d-e4f5-4a96-a305-387450af1930"
   },
   "outputs": [],
   "source": [
    "    # histogram of the height\n",
    "dfg_n.confidence_level.plot(kind='hist', color='lightblue', edgecolor='black', )\n",
    "dfg_nn.confidence_level.plot(kind='hist', color='orange', edgecolor='black', )\n",
    "plt.title('Distribution of Confidence Level of Native and Non-Native English Speakers' )\n",
    "plt.xlabel('Confidence Level', size=12)\n",
    "plt.ylabel('Frequency', size=12)\n",
    "\n",
    "\n",
    "# sns.catplot(y=\"confidence_level\", hue=\"native_bool\", kind=\"count\",\n",
    "#             palette=\"pastel\", edgecolor=\".6\",\n",
    "#             data= dfg);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "JkKo_05F_2pp",
    "outputId": "b2b56f4f-e76e-4f86-b61a-8f526213a617"
   },
   "outputs": [],
   "source": [
    "print(\"Getting results for non-imputed data\")\n",
    "dfg_n = dfg[dfg['native_bool'] == True]\n",
    "dfg_nn = dfg[dfg['native_bool'] == False]\n",
    "\n",
    "get_single_mean(dfg_n, 'confidence_level')\n",
    "get_single_mean(dfg_nn, 'confidence_level')\n",
    "print(\"The variance for native speaker overall grade score is \" + str(np.var(dfg_n.confidence_level)))\n",
    "print(\"The variance for non-native speaker overall grade score is \" + str(np.var(dfg_nn.confidence_level)))\n",
    "\n",
    "ttest_ind(dfg_n['confidence_level'], dfg_nn['confidence_level'], equal_var=False,nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "xzVqR-j9_2ps",
    "outputId": "c618bf72-dd97-40f5-f19a-39ad3703ad74"
   },
   "outputs": [],
   "source": [
    "print(\"Getting results for imputed data\")\n",
    "dfg = impute_from_distrib(dfg, 'confidence_level')\n",
    "dfg_n = dfg[dfg['native_bool'] == True]\n",
    "dfg_nn = dfg[dfg['native_bool'] == False]\n",
    "\n",
    "get_single_mean(dfg_n, 'confidence_level')\n",
    "get_single_mean(dfg_nn, 'confidence_level')\n",
    "print(\"The variance for native speaker overall grade score is \" + str(np.var(dfg_n.confidence_level)))\n",
    "print(\"The variance for non-native speaker overall grade score is \" + str(np.var(dfg_nn.confidence_level)))\n",
    "\n",
    "ttest_ind(dfg_n['confidence_level'], dfg_nn['confidence_level'], equal_var=False,nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8q9VR2_e_2px"
   },
   "source": [
    "### <a id='section_RQC_conclusion'> Research Question Conclusion On Confidence Level </a>  \n",
    "\n",
    " [- Return](#toc) \n",
    "\n",
    "After running the t-test and getting a p-value of 0.04 which is less than 0.05, we reject the null hypothesis of average identical confidence levels of non-native english speaker students and native english speaker students \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5HSaoUL_2px"
   },
   "source": [
    "## General On Multiple Factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "1UYAnp4U_2px",
    "outputId": "5750e476-5106-4753-9344-161753547a47"
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"english_fluency\", y='overall_grades', data= dfg)\n",
    "correlation = dfg['english_fluency'].corr(dfg['overall_grades'])\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ntJl3N0V_2p4",
    "outputId": "c539223c-b001-4881-e62c-db92707b42f0"
   },
   "outputs": [],
   "source": [
    "for x in dfg.columns:\n",
    "    try: \n",
    "        print(\"The correlation between \" + x + \" and overall grades is \" + str(dfg[x].corr(dfg['overall_grades'])))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# from scipy.stats import ks_2samp\n",
    "# ks_2samp(dfg_n.overall_grades, dfg_nn.overall_grades)\n",
    "# ks_2samp(dfg_n.confidence_level, dfg_nn.confidence_level)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0pDYdb2_2p-"
   },
   "outputs": [],
   "source": [
    "# df_full['ethnicity_bool'] = df_full.ethnicity == 'Hispanic or Latino/a'\n",
    "# print(\"The correlation between \" + x + \" and ethnicity bool is \" + str(df_full['native_bool'].corr(df_full['ethnicity_bool'])))\n",
    "\n",
    "# df_full[['ethnicity_bool', 'ethnicity']]\n",
    "\n",
    "\n",
    "# df_full\n",
    "\n",
    "# hmm2 = df[(df['Race'] != 'Human')  & (df['Publisher'] != 'DC Comics' )  & (df['Publisher'] != 'Marvel Comics' ) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-l_imoz6_2qM"
   },
   "source": [
    "\n",
    "\n",
    "# <a id='section_RQC_conclusion'> Research Question Get Resource </a>  \n",
    "\n",
    "[- Return](#toc) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFdrO6zr2JGI"
   },
   "source": [
    "### Question 1) Is there a significant difference between the average weekly percentage of NN and NE students who used online resources over the course of the quarter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKIRvFpm4CPl"
   },
   "source": [
    "###Question 2) Is there a significant difference between the average weekly percentage of NN and NE students who used in-person resources over the course of the quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fARMimLH5nZj"
   },
   "source": [
    "#### A note about imputation\n",
    "\n",
    "The resource columns are imputed using the impute_from_distrib function to fill in the empty response data. This was we differentiate between those students who explicitly stated that they did not use a resource and those who left the question blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "SS4xqCTYJDHU",
    "outputId": "05f64a52-1854-4789-f53e-c2fe867411a2"
   },
   "outputs": [],
   "source": [
    "for col in df_full.columns:\n",
    "    if '_resource' in col:\n",
    "      print(describe_missing(col))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-UzPMii_2qJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "for col in df_full.columns:\n",
    "    if '_resource' in col:\n",
    "        df_full = impute_from_distrib(df_full, col)\n",
    "#a copy of the imputed data is made to be used below\n",
    "df_full_2 = df_full.copy()\n",
    "df_full_2['id'] = df_full_2.index\n",
    "\n",
    "df_res_frequency = df_full.copy()\n",
    "df_res_frequency['id'] = df_full_2.index\n",
    "#print(df_res_frequency.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_w_X2vNEGJ4G"
   },
   "source": [
    "**function get_res_test:**  \n",
    "takes in a df, a list of relevant columns from the input df; a search word used to get the colums that contain it; english_fluency boolean (True for NE or False for NN). **Returns** a dataframe containing the percentage of students for each category per week. Columns include: week, in-person, online and both.\n",
    " \n",
    "note: input data must be imputed prior. Both NN and NE categories of students are calculated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7hIOM-c_2qM"
   },
   "outputs": [],
   "source": [
    "#bool english_fluency True if NE(Native Eng Speaker), False if NN(Non-Native Eng Speaker)\n",
    "\n",
    "def get_res_test(df_input, list_standard, search_word, english_fluency): \n",
    "   \n",
    "    #Get only column names we want to keep in new dataframe\n",
    "    keep_titles = list_standard\n",
    "    keep_titles += [col for col in df_input.columns if search_word in col ]\n",
    "\n",
    "    #new dataframe including columns specified in keep_titles\n",
    "    df_r = df_input[keep_titles]\n",
    "\n",
    "    #new dataframe for either native or non-native\n",
    "    if english_fluency == True:\n",
    "        df_get_resources = (df_r[df_r['native_bool'] ])\n",
    "        num_responses = df_get_resources.shape[0]\n",
    "        print(\"The total number of NE students is : \" + str(num_responses))\n",
    "    else:\n",
    "        df_get_resources = (df_r[df_r['native_bool']==False])\n",
    "        num_responses = df_get_resources.shape[0]\n",
    "        print(\"The total number of NN students is : \" + str(num_responses))\n",
    "\n",
    "    count_ip = 0; count_or = 0;count_both = 0;count_none = 0;count_either = 0;count_fr = 0\n",
    " \n",
    "    #dataframe containing proportions\n",
    "    df_per_res = pd.DataFrame()\n",
    "     \n",
    "    # Yields a tuple of column name and series for each column in the dataframe\n",
    "    #iterating through the weeks    \n",
    "    for (columnName, columnData) in df_get_resources.iteritems():\n",
    "        simple_list = [[]]\n",
    "        if '_resource' in columnName:\n",
    "            \n",
    "            #check every week for resource used per person and creating a boolean column for each \n",
    "            # online vs in-person cols\n",
    "            ip_tutor = columnData.str.contains(pat = 'A tutor')\n",
    "            ip_ta = columnData.str.contains('TA')\n",
    "            ip_peer = columnData.str.contains('My peer(s)',regex=False)\n",
    "            ol_discuss = columnData.str.contains('Discussion')\n",
    "            ol = columnData.str.contains('Stack',case=False,regex=False)|columnData.str.contains('internet',case=False,regex=False)|columnData.str.contains('Java', case = False,regex = False)|columnData.str.contains('looked up documentation')\n",
    "            \n",
    "            #redundant\n",
    "            peers_pair = columnData.str.contains('as pair programming/joint assignment submission',regex=False)&columnData.str.contains('version')\n",
    "            peers_own = columnData.str.contains('version')\n",
    "            ol_discuss_post = columnData.str.contains('I posted',regex=False)\n",
    "            ol_discuss_read = columnData.str.contains('reading responses',regex=False)\n",
    "            \n",
    "            #other categories\n",
    "            ip_friends = columnData.str.contains('friend')\n",
    "            none = columnData.str.contains('I did not seek help')\n",
    "            \n",
    "            #making a df of the boolean columns online vs in-person\n",
    "            res_bool = pd.DataFrame({'tu' : ip_tutor, 'TA' : ip_ta, 'peer': ip_peer,'disc':ol_discuss,'online':ol,'friends':ip_friends,'none':none})\n",
    "            \n",
    "            num_stud = res_bool.shape[0]\n",
    "                \n",
    "            count_ip = 0; count_or = 0;count_both = 0;count_none = 0;count_either = 0;count_fr = 0\n",
    "            list_ip =[];list_or=[];list_both=[]\n",
    "            \n",
    "            #checking each index or row for boolean value\n",
    "            for index, row in res_bool.iterrows():\n",
    "                #*********************************************print([row])\n",
    "                #*********************************************print([index])\n",
    "                \n",
    "                if row['tu']|row['TA']|row['peer']== True:\n",
    "                    count_ip += 1\n",
    "                    #*****************************************print(count_ip)\n",
    "               \n",
    "                if row['disc']|row['online'] == True:\n",
    "                    count_or += 1\n",
    "                    \n",
    "                if (row['tu']|row['TA']|row['peer']== True) & (row['disc']|row['online'])== True:\n",
    "                    count_both += 1\n",
    "                    \n",
    "                if (row['tu']|row['TA']|row['peer']== True) | (row['disc']|row['online'])|(row['tu']|row['TA']|row['peer']== True) & (row['disc']|row['online']) == True:\n",
    "                    count_either += 1\n",
    "                    \n",
    "                if (row['none']== True) == True:\n",
    "                    count_none += 1\n",
    "                if (row['friends']== True) == True:\n",
    "                    count_fr += 1\n",
    "\n",
    "            #getting the percentages of online resource, in person and both (online and in-person) \n",
    "            prop_both = (count_either/num_responses)*100\n",
    "            prop_or = (count_or/num_responses)*100\n",
    "            prop_ip = (count_ip/num_responses)*100\n",
    "            #checking proportions\n",
    "            #print(\"proportions per week \" + \"in person: \"+ str(prop_ip) +\" online: \" + str(prop_or)+ \" both: \" + str(prop_both))\n",
    "            #row to append\n",
    "            \n",
    "            #checking for the fluency to assign column names\n",
    "            if english_fluency == True:\n",
    "                new_row = {'week': columnName, 'NE in-person': prop_ip, 'NE online': prop_or, 'NE both':prop_both}\n",
    "                df_per_res = df_per_res.append(new_row,ignore_index=True)\n",
    "                df_per_res.set_index('week')\n",
    "            else: \n",
    "                new_row = {'week': columnName, 'NN in-person': prop_ip, 'NN online': prop_or, 'NN both':prop_both}\n",
    "                df_per_res = df_per_res.append(new_row,ignore_index=True)\n",
    "                df_per_res.set_index('week') \n",
    "\n",
    "   \n",
    "    return df_per_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6HNcWN-Gkkv"
   },
   "source": [
    "####A note about categorization of resources\n",
    "\n",
    "Resources were categorized into two categories: \n",
    "(1) human interaction - TA, tutor, and peers (2) non-human interaction - discussion board and online resources (3) students who used both\n",
    "\n",
    "(1)\n",
    "note: \"friends outside of class\" category was excluded in because the language being used is uknown. The \"peers (pair programming)\" and \"peers (own version)\" categories were excluded because it was assumed that students are either working on their own version, or pair programming, but not both, hence only the \"peers\" category counts.\n",
    "\n",
    "(2) \n",
    "note: specific categories of discussion board were excluded and only the general 'discussion board' category\n",
    "was included along with any other online resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FQxN6lhwyALj",
    "outputId": "0ff65149-82c7-400b-dbd5-a6b7bd206f29"
   },
   "outputs": [],
   "source": [
    "#NE \n",
    "df_ne_resource = get_res_test(df_full_2,['id', 'native_bool' ], '_resource',True)\n",
    "display(df_ne_resource)\n",
    "#NN \n",
    "df_n_resource= get_res_test(df_full_2,['id', 'native_bool' ], '_resource', False)\n",
    "\n",
    "#merging the dfs for both categories and getting the mean and variance\n",
    "result = pd.concat([df_ne_resource,df_n_resource],axis=1, join='inner')\n",
    "mean = result.mean()\n",
    "var = result.var()\n",
    "\n",
    "print('the mean of each category over all weeks: \\n' + str(mean))\n",
    "print(\"the variance of each column is: \\n\" + str(var))\n",
    "print(\"standard deviation ratio of the NN and NE both: \" + str(math.sqrt(var[3])/math.sqrt(var[0])))\n",
    "print(\"standard deviation ratio of the NN and NE in-person: \" + str(math.sqrt(var[4])/math.sqrt(var[1])))\n",
    "print(\"standard deviation ratio of the NN and NE online: \" + str(math.sqrt(var[5])/math.sqrt(var[2])))\n",
    "#independent t-test\n",
    "print(\"t-test: both online and in-person resources: \" + str(ttest_ind(df_n_resource['NN both'], df_ne_resource['NE both'],equal_var = False)))\n",
    "print(\"t-test: online resources only: \" + str(ttest_ind(df_n_resource['NN online'], df_ne_resource['NE online'],equal_var=False)))\n",
    "print(\"t-test: in-person resources only: \" + str(ttest_ind(df_n_resource['NN in-person'], df_ne_resource['NE in-person'],equal_var=False)))\n",
    "\n",
    "#colums are rearranged for better order in the graphical representation\n",
    "data = {'week':df_ne_resource['week'],'NN percentage of in-person resources':result['NN in-person'], \n",
    "        'NE percentage of in-person resources':result[\"NE in-person\"], \n",
    "        'NN percentage of online resources':result[\"NN online\"], 'NE percentage of online resources':result['NE online']}\n",
    "result_col_adjust = pd.DataFrame(data)\n",
    "result_col_adjust.set_index(df_ne_resource['week'])\n",
    "\n",
    "#to display the resulting columns\n",
    "#display(result_col_adjust)\n",
    "\n",
    "#plot style/hexcolors\n",
    "color_list = ['#229954', '#E67E22', '#72D9CE', '#EEB7A1']\n",
    "ax = result_col_adjust.plot(kind='bar', color=color_list)\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5),fontsize = 10)#loc=1, fontsize=9)\n",
    "plt.title(\"Percentage of non-native vs. native English speakers who used online and in-person resources used by\",fontsize = 12)\n",
    "plt.xlabel(\"weeks of instruction\",fontsize = 10)\n",
    "plt.ylabel(\"percentage of students (who used resource)\",fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fE0FkzJ4j48I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdhzxdKVG7OC"
   },
   "source": [
    "#### T-test and General Trends\n",
    "**Independent t-test:** where the mean we care about is percentage of students who used a resource type over 9 weeks. \n",
    "\n",
    "(3 tests) - Is there a significant difference between the average number of NN and NE students who used a) online resources b) in-person resources c) both resources simultaneously\n",
    "\n",
    "Ho: The null hypothesis is that the average mean from the two independent groups (NN and NE) are equal.\n",
    "H1: The means are not equal \n",
    "\n",
    "We cannot reject the null hypothesis\n",
    "\n",
    "**General Trends** \n",
    "Generally both categories of students seek out the in-person resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lq9fIj8k6yhI"
   },
   "source": [
    "#### Correlations Relating English Language Fluency to Grades and Resource Use Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AIls6f4l6XT"
   },
   "source": [
    "**function get_res_frequency_from_weeks:**  \n",
    "takes in an df, where every column related to resource in the df should be fully imputed  **Returns** a dataframe containing the original resource responses for every week, a concatenated column, and columns containing the frequency with which each each student in the df used a particular resource over the course of the quarter.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5aYaV1KlNQB"
   },
   "outputs": [],
   "source": [
    "#input is a fully imputed dataframe\n",
    "def get_res_frequency_from_weeks(df_input):\n",
    "    \"\"\"Gets the frequency that each student (NN and NE) used a particular resource\"\"\"\n",
    "    \n",
    "    df_input = df_input[['time', 'native_bool', 'english_fluency','gender',\n",
    "                           'confidence_level','transfer','prior_exp','wk2_resource', \n",
    "                            'wk3_resource','wk4_resource', 'wk5_resource', 'wk6_resource', \n",
    "                              'wk7_resource','wk8_resource', 'wk9_resource']]\n",
    "\n",
    "    \n",
    "    #copying each column and concatenating the resource strings for each student\n",
    "    copy_3 = df_input['wk3_resource'].copy()\n",
    "    copy_4 = df_input['wk4_resource'].copy()\n",
    "    copy_5 = df_input['wk5_resource'].copy()\n",
    "    copy_6 = df_input['wk6_resource'].copy()\n",
    "    copy_7 = df_input['wk7_resource'].copy()\n",
    "    copy_8 = df_input['wk8_resource'].copy()\n",
    "    copy_9 = df_input['wk9_resource'].copy()\n",
    "    df_input[\"concat_resource\"]= df_input['wk2_resource'].str.cat(copy_3, sep =\"**\")\n",
    "    df_input[\"concat_resource\"]= df_input[\"concat_resource\"].str.cat(copy_4, sep =\"**\")\n",
    "    df_input[\"concat_resource\"]= df_input[\"concat_resource\"].str.cat(copy_5, sep =\"**\")\n",
    "    df_input[\"concat_resource\"]= df_input[\"concat_resource\"].str.cat(copy_7, sep =\"**\")\n",
    "    df_input[\"concat_resource\"]= df_input[\"concat_resource\"].str.cat(copy_8, sep =\"**\")\n",
    "    df_input[\"concat_resource\"]= df_input[\"concat_resource\"].str.cat(copy_9, sep =\"**\")\n",
    "    \n",
    "    #applying count to each for the frequency of each resource using coun\n",
    "    df_input['google']= df_input['concat_resource'].apply(lambda x: x.count('Google'))\n",
    "    df_input['stackoverflow']= df_input['concat_resource'].apply(lambda x: x.count('overflow'))\n",
    "    df_input['online']= df_input['concat_resource'].apply(lambda x: x.count('online'))\n",
    "    df_input['discussion_board_general']= df_input['concat_resource'].apply(lambda x: x.count('Discussion'))\n",
    "    df_input['posted_to_discussion_board']= df_input['concat_resource'].apply(lambda x: x.count('I posted'))\n",
    "    df_input['read_posts_discussion_board']= df_input['concat_resource'].apply(lambda x: x.count('help from reading'))\n",
    "    df_input['friends_outside_of_class']= df_input['concat_resource'].apply(lambda x: x.count('friends'))\n",
    "    df_input['peers_from_class_general']= df_input['concat_resource'].apply(lambda x: x.count('peer'))\n",
    "    df_input['peer_helped_own_version']= df_input['concat_resource'].apply(lambda x: x.count('own version'))\n",
    "    df_input['peer_pair_programming']= df_input['concat_resource'].apply(lambda x: x.count('pair programming'))\n",
    "    df_input['tutors']= df_input['concat_resource'].apply(lambda x: x.count('tutor'))\n",
    "    df_input['Teaching_Assistants']= df_input['concat_resource'].apply(lambda x: x.count('TA'))\n",
    "    df_input['explicitly_stated_no_resource']= df_input['concat_resource'].apply(lambda x: x.count('I did not seek help or work with anyone this week'))\n",
    "    \n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "brd2uEFZlkhp",
    "outputId": "95d00267-cdac-458d-a409-0c6335f8a7b9"
   },
   "outputs": [],
   "source": [
    "#getting the resource frequency for all students, NN students and NE students separately\n",
    "# df_get_resources_nn = (df_res_frequency[df_res_frequency['native_bool']==False])\n",
    "# df_get_resources_ne = (df_res_frequency[df_res_frequency['native_bool']])\n",
    "df_res_from_weeks_all = get_res_frequency_from_weeks(df_res_frequency)\n",
    "# df_res_from_weeks_ne = get_res_frequency_from_weeks(df_get_resources_ne)\n",
    "# df_res_from_weeks_nn = get_res_frequency_from_weeks(df_get_resources_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "ZryMA7MUpZLW",
    "outputId": "f8a1e905-ec6d-4e57-ca0e-4361dd1cc5d1"
   },
   "outputs": [],
   "source": [
    "#get cols needed from grades\n",
    "df_grades_mini = df_grades[['Overall.weighted.course.percentage','Final.exam.percentage','DeID']]\n",
    "\n",
    "# inner join is the default option so should be joining correclty\n",
    "df_all_res_grades = pd.merge(left=df_grades_mini, right=df_res_from_weeks_all, left_on='DeID', right_on='id')\n",
    "\n",
    "plt.rc('figure', figsize=( 7,4)  , dpi=200)\n",
    "plt.rc('font', size=4)   # font size\n",
    "plt.xlabel(\"Values on X axis\")\n",
    "sns.heatmap(df_all_res_grades.corr(), annot = True, cmap= 'coolwarm', square=True, linewidths=.5, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3CTgE_zWvcX"
   },
   "source": [
    "### A. Slight negative correlation between TA, Tutor, and Native_bool\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtYyoBsO_2qB"
   },
   "source": [
    "#  <a id='section_interview'> INTERVIEW QUESTIONS </a> \n",
    "\n",
    "[- Return](#toc) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46pWN1INIsEV"
   },
   "source": [
    "#### A set of potential interview questions to understand more about the struggles that non-native English speakers face within the CS classroom  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ys6-EFYM_2qC"
   },
   "source": [
    "- Do you feel that you spend more time studying on CS classes outside of class than you do on other classes? Why or why not?  \n",
    "- When you do spend time out of CS classes, what categories are the hardest for you? (Readings, assignments, etc.) \n",
    "- Out of these list of resources: Which one do you feel most comfortable and why, which one has been the most helpful? \n",
    "\n",
    "### Recommendations for Future Survey Questions\n",
    "- What resource has been the most helpful for you? (Drawing from the choices of 'how did you seek help this week')\n",
    "- Out of these list of resources: Which one do you feel most comfortable and why, which one has been the most helpful? \n",
    "- Including a description of the definition of race for the Native American race category might help decrease the number of students who mark \"other,\" as they might not identify with the race categories listed. The U.S. gov definition is: \"American Indian or Alaska Native. A person having origins in any of the original peoples of **North and South America (including Central America), and who maintains tribal affiliation or community attachment**.\" The bolded part of the statement is important because people from this part of the world typically do not refer to themselves as \"American Indian or Alaskan Native\" so stating that this category includes people ALL of North, Central, and South America is important. \n",
    "https://www.carleton.edu/ira/fed-race-ethnic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fck1QGCt_2qa"
   },
   "source": [
    "[Return To Table of Contents](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfWBG7V0uuh6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kgf7xfSG_2pL"
   ],
   "name": "data_analysis_cse8A_winter2020_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
